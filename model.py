import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class ConditionalUNet(nn.Module):
    def __init__(self, params):
        super(ConditionalUNet, self).__init__()
        self.task_id = params.task_id
        
        if self.task_id == 4:  # æµé‡é¢„æµ‹ä»»åŠ¡
            self._build_traffic_unet(params)
        else:
            self._build_default_unet(params)
    
    def _build_traffic_unet(self, params):
        """ä¸ºæµé‡é¢„æµ‹ä»»åŠ¡æ„å»ºUNet - ä¿®å¤ç‰ˆæœ¬"""
        self.n_apps = 20  # åº”ç”¨æ•°é‡
        self.embedding_dim = 128  # ç¯å¢ƒåµŒå…¥ç»´åº¦
        self.seq_len = getattr(params, 'seq_len', 24)
        self.pred_len = getattr(params, 'pred_len', 12)
        
        # æ•°æ®é€šé“
        self.data_channels = self.n_apps
        self.cond_channels = self.n_apps + self.embedding_dim
        
        # æ—¶é—´åµŒå…¥
        self.time_embed_dim = 128
        self.time_embed = nn.Sequential(
            nn.Linear(1, self.time_embed_dim // 2),
            nn.SiLU(),
            nn.Linear(self.time_embed_dim // 2, self.time_embed_dim),
        )
        
        # ğŸ”§ ä¿®å¤ï¼šæ¡ä»¶å¤„ç†ç½‘ç»œ - é€‚åº”1Dæ—¶é—´åºåˆ—
        self.cond_proj = nn.Sequential(
            nn.Conv1d(self.cond_channels, 64, 3, padding=1),
            nn.GroupNorm(8, 64),
            nn.SiLU(),
            nn.Conv1d(64, 32, 3, padding=1),
            nn.GroupNorm(8, 32),
            nn.SiLU(),
            nn.AdaptiveAvgPool1d(self.pred_len)  # è°ƒæ•´åˆ°é¢„æµ‹é•¿åº¦
        )
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨1Då·ç§¯è€Œä¸æ˜¯2Då·ç§¯
        # è¾“å…¥ç»´åº¦ï¼š(B, n_apps + 32, pred_len)
        input_channels = self.data_channels + 32  # 32æ¥è‡ªæ¡ä»¶æŠ•å½±
        
        # ç¼–ç å™¨ - ä½¿ç”¨1Då·ç§¯
        self.down_blocks = nn.ModuleList([
            DownBlock1D(input_channels, 64, self.time_embed_dim),
            DownBlock1D(64, 128, self.time_embed_dim),
            DownBlock1D(128, 256, self.time_embed_dim),
        ])
        
        # ä¸­é—´å—
        self.mid_block = MiddleBlock1D(256, self.time_embed_dim)
        
        # è§£ç å™¨
        self.up_blocks = nn.ModuleList([
            UpBlock1D(256 + 256, 128, self.time_embed_dim),
            UpBlock1D(128 + 128, 64, self.time_embed_dim),
            UpBlock1D(64 + 64, 32, self.time_embed_dim),
        ])
        
        # è¾“å‡ºå±‚
        self.out_conv = nn.Sequential(
            nn.GroupNorm(8, 32),
            nn.SiLU(),
            nn.Conv1d(32, self.data_channels, 3, padding=1),
        )
    
    def _build_default_unet(self, params):
        """é»˜è®¤UNetç»“æ„"""
        self.time_embed_dim = 128
        self.time_embed = nn.Sequential(
            nn.Linear(1, self.time_embed_dim // 2),
            nn.SiLU(),
            nn.Linear(self.time_embed_dim // 2, self.time_embed_dim),
        )
        
        self.conv_in = nn.Conv2d(1, 64, 3, padding=1)
        self.conv_mid = nn.Conv2d(64, 128, 3, padding=1)
        self.conv_out = nn.Conv2d(128, 1, 3, padding=1)
    
    def forward(self, x, t, cond=None):
        if self.task_id == 4:
            return self._forward_traffic(x, t, cond)
        else:
            return self._forward_default(x, t, cond)
    
    def _forward_traffic(self, x, t, cond):
        """ğŸ”§ ä¿®å¤ï¼šæµé‡é¢„æµ‹çš„å‰å‘ä¼ æ’­ - 1Dç‰ˆæœ¬"""
        B, C, T, _ = x.shape  # (B, n_apps, pred_len, 1)
        
        # ğŸ”§ å°†4Dæ•°æ®è½¬æ¢ä¸º3Dï¼š(B, n_apps, pred_len, 1) -> (B, n_apps, pred_len)
        x = x.squeeze(-1)  # (B, n_apps, pred_len)
        
        # æ—¶é—´åµŒå…¥
        t_embed = self.time_embed(t.float().unsqueeze(-1))  # (B, time_embed_dim)
        
        # å¤„ç†æ¡ä»¶
        if cond is not None:
            # cond: (B, n_apps + embedding_dim, seq_len)
            cond_feat = self.cond_proj(cond)  # (B, 32, pred_len)
            
            # æ‹¼æ¥æ•°æ®å’Œæ¡ä»¶
            x = torch.cat([x, cond_feat], dim=1)  # (B, n_apps + 32, pred_len)
        
        # ğŸ”§ UNetå‰å‘ä¼ æ’­ - 1Dç‰ˆæœ¬
        skip_connections = []
        h = x
        
        # ç¼–ç å™¨
        for down_block in self.down_blocks:
            h = down_block(h, t_embed)
            skip_connections.append(h)
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨1Dæ± åŒ–ï¼Œå¹¶ç¡®ä¿ä¸ä¼šå˜æˆ0
            if h.shape[2] > 2:  # åªæœ‰å½“é•¿åº¦>2æ—¶æ‰æ± åŒ–
                h = F.avg_pool1d(h, 2)
        
        # ä¸­é—´å—
        h = self.mid_block(h, t_embed)
        
        # è§£ç å™¨
        for up_block in self.up_blocks:
            skip = skip_connections.pop()
            # ğŸ”§ ä¿®å¤ï¼š1Dæ’å€¼
            if h.shape[2] != skip.shape[2]:
                h = F.interpolate(h, size=skip.shape[2], mode='nearest')
            h = torch.cat([h, skip], dim=1)
            h = up_block(h, t_embed)
        
        # è¾“å‡º
        out = self.out_conv(h)  # (B, n_apps, pred_len)
        
        # ğŸ”§ è½¬æ¢å›4Dæ ¼å¼ä»¥åŒ¹é…è¾“å…¥
        out = out.unsqueeze(-1)  # (B, n_apps, pred_len, 1)
        
        return out
    
    def _forward_default(self, x, t, cond=None):
        """é»˜è®¤å‰å‘ä¼ æ’­"""
        t_embed = self.time_embed(t.float().unsqueeze(-1))
        
        h = self.conv_in(x)
        h = h + t_embed.unsqueeze(-1).unsqueeze(-1)
        h = F.relu(h)
        h = self.conv_mid(h)
        h = F.relu(h)
        out = self.conv_out(h)
        
        return out


# ğŸ”§ æ–°å¢ï¼š1Dç‰ˆæœ¬çš„ç½‘ç»œå—
class DownBlock1D(nn.Module):
    def __init__(self, in_channels, out_channels, time_embed_dim):
        super().__init__()
        self.time_mlp = nn.Linear(time_embed_dim, out_channels)
        
        self.conv1 = nn.Conv1d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv1d(out_channels, out_channels, 3, padding=1)
        
        self.norm1 = nn.GroupNorm(min(8, out_channels), out_channels)
        self.norm2 = nn.GroupNorm(min(8, out_channels), out_channels)
        
        self.activation = nn.SiLU()
    
    def forward(self, x, t_embed):
        h = self.conv1(x)
        h = self.norm1(h)
        h = self.activation(h)
        
        # æ·»åŠ æ—¶é—´åµŒå…¥
        time_emb = self.time_mlp(t_embed)  # (B, out_channels)
        h = h + time_emb.unsqueeze(-1)  # å¹¿æ’­åˆ° (B, out_channels, T)
        
        h = self.conv2(h)
        h = self.norm2(h)
        h = self.activation(h)
        
        return h


class UpBlock1D(nn.Module):
    def __init__(self, in_channels, out_channels, time_embed_dim):
        super().__init__()
        self.time_mlp = nn.Linear(time_embed_dim, out_channels)
        
        self.conv1 = nn.Conv1d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv1d(out_channels, out_channels, 3, padding=1)
        
        self.norm1 = nn.GroupNorm(min(8, out_channels), out_channels)
        self.norm2 = nn.GroupNorm(min(8, out_channels), out_channels)
        
        self.activation = nn.SiLU()
    
    def forward(self, x, t_embed):
        h = self.conv1(x)
        h = self.norm1(h)
        h = self.activation(h)
        
        # æ·»åŠ æ—¶é—´åµŒå…¥
        time_emb = self.time_mlp(t_embed)
        h = h + time_emb.unsqueeze(-1)
        
        h = self.conv2(h)
        h = self.norm2(h)
        h = self.activation(h)
        
        return h


class MiddleBlock1D(nn.Module):
    def __init__(self, channels, time_embed_dim):
        super().__init__()
        self.time_mlp = nn.Linear(time_embed_dim, channels)
        
        self.conv1 = nn.Conv1d(channels, channels, 3, padding=1)
        self.conv2 = nn.Conv1d(channels, channels, 3, padding=1)
        
        self.norm1 = nn.GroupNorm(min(8, channels), channels)
        self.norm2 = nn.GroupNorm(min(8, channels), channels)
        
        self.activation = nn.SiLU()
    
    def forward(self, x, t_embed):
        h = self.conv1(x)
        h = self.norm1(h)
        h = self.activation(h)
        
        # æ·»åŠ æ—¶é—´åµŒå…¥
        time_emb = self.time_mlp(t_embed)
        h = h + time_emb.unsqueeze(-1)
        
        h = self.conv2(h)
        h = self.norm2(h)
        h = self.activation(h)
        
        return h


# ä¿ç•™åŸæœ‰çš„2Då—ä»¥å…¼å®¹å…¶ä»–ä»»åŠ¡
class DownBlock(nn.Module):
    def __init__(self, in_channels, out_channels, time_embed_dim):
        super().__init__()
        self.time_mlp = nn.Linear(time_embed_dim, out_channels)
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        
        self.norm1 = nn.GroupNorm(min(8, out_channels), out_channels)
        self.norm2 = nn.GroupNorm(min(8, out_channels), out_channels)
        
        self.activation = nn.SiLU()
    
    def forward(self, x, t_embed):
        h = self.conv1(x)
        h = self.norm1(h)
        h = self.activation(h)
        
        time_emb = self.time_mlp(t_embed)
        h = h + time_emb.unsqueeze(-1).unsqueeze(-1)
        
        h = self.conv2(h)
        h = self.norm2(h)
        h = self.activation(h)
        
        return h


class UpBlock(nn.Module):
    def __init__(self, in_channels, out_channels, time_embed_dim):
        super().__init__()
        self.time_mlp = nn.Linear(time_embed_dim, out_channels)
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        
        self.norm1 = nn.GroupNorm(min(8, out_channels), out_channels)
        self.norm2 = nn.GroupNorm(min(8, out_channels), out_channels)
        
        self.activation = nn.SiLU()
    
    def forward(self, x, t_embed):
        h = self.conv1(x)
        h = self.norm1(h)
        h = self.activation(h)
        
        time_emb = self.time_mlp(t_embed)
        h = h + time_emb.unsqueeze(-1).unsqueeze(-1)
        
        h = self.conv2(h)
        h = self.norm2(h)
        h = self.activation(h)
        
        return h


class MiddleBlock(nn.Module):
    def __init__(self, channels, time_embed_dim):
        super().__init__()
        self.time_mlp = nn.Linear(time_embed_dim, channels)
        
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        
        self.norm1 = nn.GroupNorm(min(8, channels), channels)
        self.norm2 = nn.GroupNorm(min(8, channels), channels)
        
        self.activation = nn.SiLU()
    
    def forward(self, x, t_embed):
        h = self.conv1(x)
        h = self.norm1(h)
        h = self.activation(h)
        
        time_emb = self.time_mlp(t_embed)
        h = h + time_emb.unsqueeze(-1).unsqueeze(-1)
        
        h = self.conv2(h)
        h = self.norm2(h)
        h = self.activation(h)
        
        return h
