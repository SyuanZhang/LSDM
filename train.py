import os
from pathlib import Path
import torch
from torch.cuda import device_count
from torch.multiprocessing import spawn
from torch.nn.parallel import DistributedDataParallel

from argparse import ArgumentParser

from params import all_params
from learner import tfdiffLearner
# ä¿®æ”¹å¯¼å…¥ï¼Œä½¿ç”¨æ”¯æŒmaskçš„æ¨¡å‹
try:
    from wifi_model import tfdiff_WiFi, MaskedDiffusionTrainer, MaskedDiffusionLoss, MaskedDiffusionConfig
    MASK_MODEL_AVAILABLE = True
except ImportError:
    from wifi_model import tfdiff_WiFi
    MASK_MODEL_AVAILABLE = False
    print("âš ï¸ Masked model not available, using original model")

from dataset import from_path
from model import ConditionalUNet

def _get_free_port():
    import socketserver
    with socketserver.TCPServer(('localhost', 0), None) as s:
        return s.server_address[1]

def _train_impl(replica_id, model, dataset, params):
    opt = torch.optim.AdamW(
        model.parameters(),
        lr=params.learning_rate,
        weight_decay=1e-6, eps=1e-8, betas=(0.9, 0.999)
    )
    learner = tfdiffLearner(params.log_dir, params.model_dir, model, dataset, opt, params)
    learner.is_master = (replica_id == 0)
    learner.restore_from_checkpoint()

    for g in opt.param_groups:
        g['lr'] = params.learning_rate

    learner.train(max_iter=params.max_iter)

def safe_save_model(model_state, save_path, description="model"):
    """å®‰å…¨ä¿å­˜æ¨¡å‹çš„å‡½æ•°"""
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        save_dir = Path(save_path).parent
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶åé¿å…å†²çª
        temp_path = str(save_path) + '.tmp'
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        torch.save(model_state, temp_path)
        
        # åŸå­æ€§æ›¿æ¢
        if Path(save_path).exists():
            Path(save_path).unlink()  # åˆ é™¤æ—§æ–‡ä»¶
        Path(temp_path).rename(save_path)  # é‡å‘½åä¸´æ—¶æ–‡ä»¶
        
        print(f"ğŸ† {description} saved: {save_path}")
        return True
        
    except Exception as e:
        print(f"âš ï¸ Failed to save {description}: {e}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_path = str(save_path) + '.tmp'
        if Path(temp_path).exists():
            try:
                Path(temp_path).unlink()
            except:
                pass
        return False

def verify_dataset_split(dataset):
    """æ­£ç¡®éªŒè¯æ•°æ®é›†ç”¨æˆ·åˆ’åˆ†"""
    print("ğŸ” éªŒè¯æ•°æ®é›†åˆ’åˆ†æ–¹å¼...")
    
    all_users = set()
    batch_count = 0
    total_samples = 0
    
    try:
        for batch_idx, batch in enumerate(dataset):
            if isinstance(batch, dict):
                # å¤„ç†ä¸åŒçš„ç”¨æˆ·IDå­—æ®µå
                user_ids = None
                if 'user_ids' in batch:
                    user_ids = batch['user_ids']
                elif 'user_id' in batch:
                    user_ids = batch['user_id']
                    if not isinstance(user_ids, (list, tuple)):
                        user_ids = [user_ids]
                
                if user_ids is not None:
                    if isinstance(user_ids, torch.Tensor):
                        user_ids = user_ids.tolist()
                    elif not isinstance(user_ids, (list, tuple)):
                        user_ids = [user_ids]
                    
                    all_users.update(user_ids)
                    total_samples += len(user_ids)
                    
                    # åªæ˜¾ç¤ºå‰3ä¸ªæ‰¹æ¬¡çš„è¯¦æƒ…
                    if batch_idx < 3:
                        print(f"  â€¢ Batch {batch_idx}: {len(user_ids)} users = {user_ids}")
                    elif batch_idx == 3:
                        print(f"  â€¢ ... (ç»§ç»­å¤„ç†å‰©ä½™æ‰¹æ¬¡)")
                
                batch_count += 1
                
                # é˜²æ­¢æ•°æ®é›†å¤ªå¤§æ—¶çš„æ— é™å¾ªç¯
                if batch_count >= 100:  
                    print(f"  â€¢ å·²æ£€æŸ¥ {batch_count} ä¸ªæ‰¹æ¬¡ï¼Œåœæ­¢éªŒè¯ä»¥èŠ‚çœæ—¶é—´")
                    break
        
        print(f"âœ… æ•°æ®é›†éªŒè¯å®Œæˆ:")
        print(f"  â€¢ æ£€æŸ¥æ‰¹æ¬¡æ•°: {batch_count}")
        print(f"  â€¢ æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"  â€¢ å”¯ä¸€ç”¨æˆ·æ•°: {len(all_users)}")
        if all_users:
            print(f"  â€¢ ç”¨æˆ·IDèŒƒå›´: [{min(all_users)}, {max(all_users)}]")
            # æ˜¾ç¤ºä¸€äº›ç”¨æˆ·IDæ ·æœ¬
            user_sample = sorted(list(all_users))[:20]
            print(f"  â€¢ ç”¨æˆ·IDç¤ºä¾‹: {user_sample}{'...' if len(all_users) > 20 else ''}")
        
        return all_users, total_samples, batch_count
        
    except Exception as e:
        print(f"âš ï¸ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return set(), 0, 0

def analyze_data_format(sample_batch, params):
    """è¯¦ç»†åˆ†ææ•°æ®æ ¼å¼å¹¶è¿”å›æ­£ç¡®çš„ç»´åº¦ä¿¡æ¯"""
    print("ğŸ” è¯¦ç»†åˆ†ææ•°æ®æ ¼å¼...")
    
    if not isinstance(sample_batch, dict) or 'data' not in sample_batch:
        print("âŒ æ— æ•ˆçš„æ‰¹æ¬¡æ ¼å¼")
        return None
    
    data = sample_batch['data']
    print(f"ğŸ“Š æ•°æ®å½¢çŠ¶åˆ†æ: {data.shape}")
    
    batch_size = data.shape[0]
    
    # è·å–å‚æ•°é…ç½®
    pred_len = getattr(params, 'pred_len', 20)
    total_seq_len = getattr(params, 'total_seq_len', 168)
    input_dim = getattr(params, 'input_dim', 20)
    
    format_info = {
        'batch_size': batch_size,
        'original_shape': data.shape,
        'time_dim_index': None,
        'time_length': None,
        'feature_dim_index': None,
        'feature_length': None,
        'format_type': None
    }
    
    if data.dim() == 2:  # [B, F]
        print("  â€¢ æ ¼å¼: [Batch, Features]")
        format_info['format_type'] = '2D'
        format_info['feature_dim_index'] = 1
        format_info['feature_length'] = data.shape[1]
        print("  âš ï¸ 2Dæ•°æ®æ— æ³•åº”ç”¨æ—¶åºmask")
        
    elif data.dim() == 3:  # [B, T, F] 
        print("  â€¢ æ ¼å¼: [Batch, Time, Features]")
        format_info['format_type'] = '3D'
        format_info['time_dim_index'] = 1
        format_info['time_length'] = data.shape[1]
        format_info['feature_dim_index'] = 2
        format_info['feature_length'] = data.shape[2]
        
    elif data.dim() == 4:  # [B, T, F, 1] æˆ– [B, C, T, F]
        dim1, dim2, dim3 = data.shape[1], data.shape[2], data.shape[3]
        
        # åˆ¤æ–­æ ¼å¼
        if dim3 == 1:  # [B, T, F, 1] æ ¼å¼
            print(f"  â€¢ æ ¼å¼: [Batch={batch_size}, Time={dim1}, Features={dim2}, Channel={dim3}]")
            format_info['format_type'] = '4D_BTFC'
            format_info['time_dim_index'] = 1
            format_info['time_length'] = dim1
            format_info['feature_dim_index'] = 2
            format_info['feature_length'] = dim2
            
        elif dim1 == 1:  # [B, 1, T, F] æ ¼å¼
            print(f"  â€¢ æ ¼å¼: [Batch={batch_size}, Channel={dim1}, Time={dim2}, Features={dim3}]")
            format_info['format_type'] = '4D_BCTF'
            format_info['time_dim_index'] = 2
            format_info['time_length'] = dim2
            format_info['feature_dim_index'] = 3
            format_info['feature_length'] = dim3
            
        else:  # é€šç”¨ [B, C, T, F] æ ¼å¼ï¼Œéœ€è¦å¯å‘å¼åˆ¤æ–­
            # æ ¹æ®é…ç½®å‚æ•°æ¨æ–­
            if dim1 == pred_len and dim2 == input_dim:
                print(f"  â€¢ æ¨æ–­æ ¼å¼: [Batch={batch_size}, Time={dim1}, Features={dim2}, Extra={dim3}]")
                format_info['format_type'] = '4D_BTFC'
                format_info['time_dim_index'] = 1
                format_info['time_length'] = dim1
                format_info['feature_dim_index'] = 2
                format_info['feature_length'] = dim2
            elif dim2 == pred_len and dim3 == input_dim:
                print(f"  â€¢ æ¨æ–­æ ¼å¼: [Batch={batch_size}, Channel={dim1}, Time={dim2}, Features={dim3}]")
                format_info['format_type'] = '4D_BCTF'
                format_info['time_dim_index'] = 2
                format_info['time_length'] = dim2
                format_info['feature_dim_index'] = 3
                format_info['feature_length'] = dim3
            else:
                print(f"  âš ï¸ æ— æ³•ç¡®å®šç»´åº¦å«ä¹‰ï¼Œä½¿ç”¨é»˜è®¤: Timeåœ¨ç´¢å¼•2")
                format_info['format_type'] = '4D_BCTF'
                format_info['time_dim_index'] = 2
                format_info['time_length'] = dim2
                format_info['feature_dim_index'] = 3
                format_info['feature_length'] = dim3
                
    elif data.dim() == 5:  # [B, C, T, F, E]
        print(f"  â€¢ æ ¼å¼: [Batch={batch_size}, C={data.shape[1]}, Time={data.shape[2]}, F={data.shape[3]}, E={data.shape[4]}]")
        format_info['format_type'] = '5D'
        format_info['time_dim_index'] = 2
        format_info['time_length'] = data.shape[2]
        format_info['feature_dim_index'] = 3
        format_info['feature_length'] = data.shape[3]
        
    else:
        print(f"  âŒ ä¸æ”¯æŒçš„ç»´åº¦: {data.dim()}")
        return None
    
    # éªŒè¯æ—¶é—´é•¿åº¦
    if format_info['time_length'] is not None:
        print(f"  âœ… æ£€æµ‹åˆ°æ—¶é—´ç»´åº¦: ç´¢å¼•{format_info['time_dim_index']}, é•¿åº¦{format_info['time_length']}")
        mask_length = getattr(params, 'mask_length', pred_len)
        if mask_length >= format_info['time_length']:
            print(f"  âš ï¸ Maské•¿åº¦({mask_length}) >= æ—¶é—´é•¿åº¦({format_info['time_length']})")
        else:
            print(f"  âœ… Maské•¿åº¦({mask_length})é…ç½®æ­£å¸¸")
    
    return format_info

def create_smart_mask(batch_size, format_info, mask_length, device, strategy='prefix'):
    """æ ¹æ®æ•°æ®æ ¼å¼æ™ºèƒ½åˆ›å»ºmask"""
    if format_info is None or format_info['time_length'] is None:
        print("âŒ æ— æ³•åˆ›å»ºmaskï¼šæ—¶é—´ç»´åº¦æœªçŸ¥")
        return None
    
    seq_len = format_info['time_length']
    time_dim = format_info['time_dim_index']
    
    # è°ƒæ•´maské•¿åº¦
    adjusted_mask_length = min(mask_length, seq_len - 1)
    if adjusted_mask_length != mask_length:
        print(f"ğŸ”§ Maské•¿åº¦è°ƒæ•´: {mask_length} -> {adjusted_mask_length}")
    
    # åˆ›å»ºåŸºç¡€mask [B, T]
    mask = torch.zeros(batch_size, seq_len, dtype=torch.bool, device=device)
    
    if strategy == 'prefix':
        mask[:, :adjusted_mask_length] = True
    elif strategy == 'suffix':
        mask[:, -adjusted_mask_length:] = True
    elif strategy == 'random':
        for b in range(batch_size):
            indices = torch.randperm(seq_len)[:adjusted_mask_length]
            mask[b, indices] = True
    
    return mask, adjusted_mask_length

def apply_smart_mask(data, mask, format_info, mask_value=0.0):
    """æ ¹æ®æ•°æ®æ ¼å¼æ™ºèƒ½åº”ç”¨mask"""
    if format_info is None or mask is None:
        return data
    
    masked_data = data.clone()
    time_dim = format_info['time_dim_index']
    
    try:
        if format_info['format_type'] == '2D':
            print("âš ï¸ 2Dæ•°æ®è·³è¿‡maskåº”ç”¨")
            return masked_data
            
        elif format_info['format_type'] == '3D':  # [B, T, F]
            # mask: [B, T] -> [B, T, F]
            mask_expanded = mask.unsqueeze(-1).expand_as(data)
            masked_data[mask_expanded] = mask_value
            
        elif format_info['format_type'] == '4D_BTFC':  # [B, T, F, 1]
            # mask: [B, T] -> [B, T, F, 1]
            mask_expanded = mask.unsqueeze(-1).unsqueeze(-1).expand_as(data)
            masked_data[mask_expanded] = mask_value
            
        elif format_info['format_type'] == '4D_BCTF':  # [B, C, T, F]
            # mask: [B, T] -> [B, C, T, F]
            mask_expanded = mask.unsqueeze(1).unsqueeze(-1).expand_as(data)
            masked_data[mask_expanded] = mask_value
            
        elif format_info['format_type'] == '5D':  # [B, C, T, F, E]
            # mask: [B, T] -> [B, C, T, F, E]
            mask_expanded = mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1).expand_as(data)
            masked_data[mask_expanded] = mask_value
            
        else:
            print(f"âš ï¸ æœªçŸ¥æ ¼å¼ç±»å‹: {format_info['format_type']}")
            return masked_data
            
    except Exception as e:
        print(f"âŒ Maskåº”ç”¨å¤±è´¥: {e}")
        return masked_data
    
    return masked_data

def compute_smart_masked_loss(pred, target, mask, format_info, loss_fn):
    """æ ¹æ®æ•°æ®æ ¼å¼æ™ºèƒ½è®¡ç®—maskæŸå¤±"""
    if format_info is None or mask is None:
        return loss_fn(pred, target)
    
    try:
        # æ ¹æ®æ ¼å¼æ‰©å±•mask
        if format_info['format_type'] == '3D':  # [B, T, F]
            mask_expanded = mask.unsqueeze(-1).expand_as(pred)
        elif format_info['format_type'] == '4D_BTFC':  # [B, T, F, 1]
            mask_expanded = mask.unsqueeze(-1).unsqueeze(-1).expand_as(pred)
        elif format_info['format_type'] == '4D_BCTF':  # [B, C, T, F]
            mask_expanded = mask.unsqueeze(1).unsqueeze(-1).expand_as(pred)
        elif format_info['format_type'] == '5D':  # [B, C, T, F, E]
            mask_expanded = mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1).expand_as(pred)
        else:
            return loss_fn(pred, target)
        
        # è®¡ç®—maskåŒºåŸŸçš„æŸå¤±
        if mask_expanded.sum() > 0:
            masked_pred = pred[mask_expanded]
            masked_target = target[mask_expanded]
            return loss_fn(masked_pred, masked_target)
        else:
            return torch.tensor(0.0, device=pred.device, requires_grad=True)
            
    except Exception as e:
        print(f"âš ï¸ MaskæŸå¤±è®¡ç®—å¤±è´¥: {e}")
        return loss_fn(pred, target)

def train_with_mask(params):
    """æ”¯æŒmaskçš„è®­ç»ƒå‡½æ•°"""
    torch.backends.cudnn.benchmark = True
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print(f"ğŸ–¥ï¸  Using device: {device}")
    print(f"ğŸ­ Mask-aware training enabled!")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    model_dir = Path(params.model_dir)
    log_dir = Path(params.log_dir)
    model_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # å®šä¹‰checkpointç›®å½•å’Œæ–‡ä»¶è·¯å¾„
    checkpoint_dir = model_dir
    best_model_path = checkpoint_dir / 'best_masked_model.pt'
    final_model_path = checkpoint_dir / 'masked_traffic_model_final.pt'
    
    print(f"ğŸ“ Directories:")
    print(f"  â€¢ Model dir: {model_dir}")
    print(f"  â€¢ Log dir: {log_dir}")
    
    # åˆ›å»ºæ•°æ®é›†
    print("ğŸ“Š Creating dataset...")
    dataset = from_path(params)
    
    # éªŒè¯æ•°æ®é›†
    all_users, total_samples, batch_count = verify_dataset_split(dataset)
    
    if not all_users:
        print("âŒ è­¦å‘Š: æ— æ³•è·å–ç”¨æˆ·ä¿¡æ¯ï¼Œå¯èƒ½ä½¿ç”¨äº†é”™è¯¯çš„æ•°æ®é›†æ ¼å¼")
        print("   å°†ç»§ç»­è®­ç»ƒä½†æ— æ³•éªŒè¯ç”¨æˆ·åˆ’åˆ†...")
    
    # åˆ†ææ•°æ®æ ¼å¼
    print("ğŸ” åˆ†ææ•°æ®æ ¼å¼...")
    try:
        sample_batch = next(iter(dataset))
        format_info = analyze_data_format(sample_batch, params)
        if format_info is None:
            print("âŒ æ•°æ®æ ¼å¼åˆ†æå¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹è®­ç»ƒ")
            return train_original(params)
    except Exception as e:
        print(f"âŒ æ— æ³•è·å–æ ·æœ¬æ•°æ®: {e}")
        return train_original(params)
    
    # åˆ›å»ºæ”¯æŒmaskçš„æ¨¡å‹
    print("ğŸ—ï¸ Creating masked diffusion model...")
    
    # å¦‚æœæœ‰å¯ç”¨çš„maskæ¨¡å‹ï¼Œä½¿ç”¨ä¸“é—¨çš„è®­ç»ƒå™¨
    if MASK_MODEL_AVAILABLE:
        # åˆ›å»ºé…ç½®
        config = MaskedDiffusionConfig()
        
        # ä»paramsæ›´æ–°é…ç½®
        config.input_dim = getattr(params, 'input_dim', 20)
        config.hidden_dim = getattr(params, 'hidden_dim', 256)
        config.cond_dim = getattr(params, 'cond_dim', 148)
        config.device = device
        config.batch_size = params.batch_size
        config.learning_rate = getattr(params, 'learning_rate', 1e-4)
        config.seq_len = format_info['time_length']
        
        # Maskç›¸å…³é…ç½®
        config.mask_length = getattr(params, 'mask_length', params.pred_len)
        config.mask_strategies = getattr(params, 'mask_strategies', ['prefix'])
        config.mask_weight = getattr(params, 'mask_weight', 1.0)
        config.unmask_weight = getattr(params, 'unmask_weight', 0.1)
        
        print(f"ğŸ­ Mask Configuration:")
        print(f"  â€¢ Mask length: {config.mask_length}")
        print(f"  â€¢ Mask strategies: {config.mask_strategies}")
        print(f"  â€¢ Mask weight: {config.mask_weight}")
        print(f"  â€¢ Unmask weight: {config.unmask_weight}")
        
        # åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
        net = tfdiff_WiFi(config).to(device)
        trainer = MaskedDiffusionTrainer(net, config)
        
        print(f"ğŸ“Š Model parameters: {sum(p.numel() for p in net.parameters()):,}")
        
        # ä½¿ç”¨ä¸“é—¨çš„è®­ç»ƒå™¨è®­ç»ƒ
        return train_with_masked_trainer(trainer, dataset, config, checkpoint_dir, params.max_iter)
    
    else:
        # å›é€€åˆ°åŸå§‹è®­ç»ƒæ–¹å¼ï¼Œä½†é›†æˆmaskåŠŸèƒ½
        net = ConditionalUNet(params).to(device)
        print(f"ğŸ“Š Model parameters: {sum(p.numel() for p in net.parameters()):,}")
        
        # ä½¿ç”¨ä¿®æ”¹åçš„è®­ç»ƒé€»è¾‘
        return train_with_manual_mask(net, dataset, params, device, checkpoint_dir, format_info)

def train_with_masked_trainer(trainer, dataset, config, checkpoint_dir, max_iter):
    """ä½¿ç”¨ä¸“é—¨çš„maskè®­ç»ƒå™¨è¿›è¡Œè®­ç»ƒ"""
    print("ğŸš€ Starting mask-aware training with specialized trainer...")
    
    best_loss = float('inf')
    iteration = 0
    loss_history = []
    
    try:
        data_loader = iter(dataset)
    except Exception as e:
        print(f"âŒ Error creating data loader: {e}")
        return
    
    while iteration < max_iter:
        for batch_idx, batch in enumerate(data_loader):
            if iteration >= max_iter:
                print(f"ğŸ Reached maximum iterations ({max_iter})")
                break
            
            try:
                # ä½¿ç”¨ä¸“é—¨çš„è®­ç»ƒå™¨è¿›è¡Œè®­ç»ƒæ­¥éª¤
                total_loss, mask_loss, unmask_loss = trainer.train_step(batch)
                
                if total_loss == float('inf'):
                    continue
                
                loss_history.append(total_loss)
                
                # æ›´æ–°æœ€ä½³æŸå¤±
                if total_loss < best_loss:
                    best_loss = total_loss
                    model_state = {
                        'model_state_dict': trainer.model.state_dict(),
                        'optimizer_state_dict': trainer.optimizer.state_dict(),
                        'scheduler_state_dict': trainer.scheduler.state_dict(),
                        'total_loss': total_loss,
                        'mask_loss': mask_loss,
                        'unmask_loss': unmask_loss,
                        'iteration': iteration,
                        'best_loss': best_loss,
                    }
                    safe_save_model(model_state, checkpoint_dir / 'best_masked_model.pt', "Best masked model")
                
                # æ‰“å°è®­ç»ƒè¿›åº¦
                if iteration % 1000 == 0:
                    current_lr = trainer.optimizer.param_groups[0]['lr']
                    print(f"Iter {iteration:4d}/{max_iter} | "
                          f"Total: {total_loss:.6f} | "
                          f"Mask: {mask_loss:.6f} | "
                          f"Unmask: {unmask_loss:.6f} | "
                          f"Best: {best_loss:.6f} | "
                          f"LR: {current_lr:.2e}")
                
                # å®šæœŸä¿å­˜checkpoint
                if iteration > 0 and iteration % 5000 == 0:
                    checkpoint_path = checkpoint_dir / f"masked_checkpoint_iter_{iteration}.pt"
                    checkpoint_state = {
                        'iteration': iteration,
                        'model_state_dict': trainer.model.state_dict(),
                        'optimizer_state_dict': trainer.optimizer.state_dict(),
                        'scheduler_state_dict': trainer.scheduler.state_dict(),
                        'total_loss': total_loss,
                        'mask_loss': mask_loss,
                        'unmask_loss': unmask_loss,
                        'loss_history': loss_history,
                        'best_loss': best_loss,
                    }
                    safe_save_model(checkpoint_state, checkpoint_path, f"Masked checkpoint {iteration}")
                
                iteration += 1
                
            except Exception as e:
                print(f"âŒ Error in iteration {iteration}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨
        try:
            data_loader = iter(dataset)
        except Exception as e:
            print(f"âŒ Error recreating data loader: {e}")
            break
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_state = {
        'model_state_dict': trainer.model.state_dict(),
        'optimizer_state_dict': trainer.optimizer.state_dict(),
        'scheduler_state_dict': trainer.scheduler.state_dict(),
        'loss_history': loss_history,
        'best_loss': best_loss,
        'final_loss': loss_history[-1] if loss_history else float('inf'),
        'total_iterations': iteration,
    }
    
    safe_save_model(final_model_state, checkpoint_dir / 'masked_traffic_model_final.pt', "Final masked model")
    
    print(f"\nğŸ Masked training completed!")
    print(f"  â€¢ Total iterations: {iteration}")
    print(f"  â€¢ Best loss: {best_loss:.6f}")
    
    if len(loss_history) > 10:
        try:
            plot_training_loss(loss_history, checkpoint_dir, prefix="masked_")
        except Exception as e:
            print(f"âš ï¸ Could not save loss plot: {e}")

def train_with_manual_mask(net, dataset, params, device, checkpoint_dir, format_info):
    """ä½¿ç”¨æ‰‹åŠ¨maskå®ç°çš„è®­ç»ƒï¼ˆå®Œå…¨ä¿®å¤ç‰ˆæœ¬ï¼‰"""
    print("ğŸš€ Starting training with manual mask implementation...")
    
    # ä¼˜åŒ–å™¨è®¾ç½®
    optimizer = torch.optim.AdamW(
        net.parameters(), 
        lr=params.learning_rate, 
        weight_decay=1e-4
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, 
        T_max=params.max_iter, 
        eta_min=1e-7
    )
    
    # æŸå¤±å‡½æ•°
    mse_loss = torch.nn.MSELoss()
    l1_loss = torch.nn.L1Loss()
    
    def combined_loss(pred, target):
        mse = mse_loss(pred, target)
        l1 = l1_loss(pred, target)
        return mse + 0.1 * l1
    
    # è®­ç»ƒå˜é‡
    net.train()
    iteration = 0
    best_loss = float('inf')
    loss_history = []
    mask_length = getattr(params, 'mask_length', params.pred_len)
    mask_strategy = getattr(params, 'mask_strategy', 'prefix')
    
    # æ—©æœŸåœæ­¢å‚æ•°
    patience = getattr(params, 'early_stopping_patience', 200)
    no_improve_count = 0
    best_loss_iteration = 0
    
    print(f"ğŸ“‹ Training parameters:")
    print(f"  â€¢ Learning rate: {params.learning_rate}")
    print(f"  â€¢ Max iterations: {params.max_iter}")
    print(f"  â€¢ Batch size: {params.batch_size}")
    print(f"  â€¢ Mask length: {mask_length}")
    print(f"  â€¢ Mask strategy: {mask_strategy}")
    print(f"  â€¢ Data format: {format_info['format_type']}")
    print(f"  â€¢ Time dimension: index {format_info['time_dim_index']}, length {format_info['time_length']}")
    print(f"  â€¢ Early stopping patience: {patience}")
    
    # è®­ç»ƒå¾ªç¯
    while iteration < params.max_iter:
        try:
            data_loader = iter(dataset)
        except Exception as e:
            print(f"âŒ Error creating data loader: {e}")
            break
        
        for batch_idx, batch in enumerate(data_loader):
            if iteration >= params.max_iter:
                break
            
            try:
                # è·å–æ•°æ®
                if isinstance(batch, dict):
                    data = batch['data'].to(device)
                    cond = batch['cond'].to(device) if 'cond' in batch else None
                else:
                    data = batch[0].to(device)
                    cond = batch[1].to(device) if len(batch) > 1 else None
                
                # è°ƒè¯•ä¿¡æ¯ï¼šç¬¬ä¸€æ¬¡è¿­ä»£æ—¶æ‰“å°è¯¦ç»†ä¿¡æ¯
                if iteration == 0:
                    print(f"ğŸ” Debug info - Batch {batch_idx}:")
                    print(f"  â€¢ Data shape: {data.shape}")
                    print(f"  â€¢ Data type: {data.dtype}")
                    print(f"  â€¢ Data range: [{data.min().item():.4f}, {data.max().item():.4f}]")
                    if cond is not None:
                        print(f"  â€¢ Cond shape: {cond.shape}")
                        print(f"  â€¢ Cond range: [{cond.min().item():.4f}, {cond.max().item():.4f}]")
                
                # éªŒè¯æ•°æ®å½¢çŠ¶ä¸åˆ†æç»“æœä¸€è‡´
                if data.shape != format_info['original_shape']:
                    print(f"âš ï¸ æ•°æ®å½¢çŠ¶ä¸ä¸€è‡´! æœŸæœ›: {format_info['original_shape']}, å®é™…: {data.shape}")
                    # é‡æ–°åˆ†æå½“å‰æ‰¹æ¬¡
                    temp_format = analyze_data_format({'data': data}, params)
                    if temp_format:
                        format_info = temp_format
                
                batch_size = data.shape[0]
                
                # åˆ›å»ºæ™ºèƒ½mask
                mask, adjusted_mask_length = create_smart_mask(
                    batch_size, format_info, mask_length, device, mask_strategy
                )
                
                if mask is None:
                    print(f"âš ï¸ æ— æ³•åˆ›å»ºmaskï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                    continue
                
                # éªŒè¯maskåˆ›å»º
                if iteration == 0:
                    print(f"  â€¢ Mask shape: {mask.shape}")
                    print(f"  â€¢ Adjusted mask length: {adjusted_mask_length}")
                    print(f"  â€¢ Mask ratio: {mask.float().mean().item():.3f}")
                
                # åŸå§‹æ•°æ®ï¼ˆç›®æ ‡ï¼‰
                target_data = data.clone()
                
                # åº”ç”¨æ™ºèƒ½mask
                masked_data = apply_smart_mask(data, mask, format_info)
                
                # éªŒè¯maskåº”ç”¨
                if iteration == 0:
                    mask_applied = not torch.equal(data, masked_data)
                    print(f"  â€¢ Mask applied successfully: {mask_applied}")
                    if mask_applied:
                        diff_count = (data != masked_data).sum().item()
                        total_elements = data.numel()
                        diff_ratio = diff_count / total_elements
                        print(f"  â€¢ Masked elements: {diff_count}/{total_elements} ({diff_ratio:.3f})")
                
                # æ‰©æ•£æ¨¡å‹è®­ç»ƒæ­¥éª¤
                t = torch.randint(0, 1000, (batch_size,), device=device)
                
                # å™ªå£°è°ƒåº¦
                beta_start, beta_end = 1e-4, 0.02
                betas = torch.linspace(beta_start, beta_end, 1000, device=device)
                alphas = 1. - betas
                alphas_cumprod = torch.cumprod(alphas, dim=0)
                
                # ç”Ÿæˆå™ªå£°å¹¶æ·»åŠ åˆ°maskæ•°æ®
                noise = torch.randn_like(masked_data)
                sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod[t])
                sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod[t])
                
                # é‡å¡‘ç³»æ•°ä»¥åŒ¹é…æ•°æ®ç»´åº¦
                shape = [-1] + [1] * (masked_data.dim() - 1)
                sqrt_alphas_cumprod = sqrt_alphas_cumprod.view(shape)
                sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.view(shape)
                
                # æ·»åŠ å™ªå£°
                noisy_data = sqrt_alphas_cumprod * masked_data + sqrt_one_minus_alphas_cumprod * noise
                
                # å‰å‘ä¼ æ’­
                optimizer.zero_grad()
                predicted = net(noisy_data, t, cond)
                
                # æ™ºèƒ½è®¡ç®—æŸå¤±
                mask_loss = compute_smart_masked_loss(predicted, target_data, mask, format_info, combined_loss)
                global_loss = combined_loss(predicted, target_data) * 0.1
                total_loss = mask_loss + global_loss
                
                # L2æ­£åˆ™åŒ–
                l2_reg = 0
                for param in net.parameters():
                    l2_reg += torch.norm(param, 2)
                total_loss += 1e-5 * l2_reg
                
                # åå‘ä¼ æ’­
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)
                optimizer.step()
                scheduler.step()
                
                # è®°å½•æŸå¤±
                current_loss = total_loss.item()
                loss_history.append(current_loss)
                
                # æ—©æœŸåœæ­¢é€»è¾‘
                if current_loss < best_loss:
                    best_loss = current_loss
                    best_loss_iteration = iteration
                    no_improve_count = 0
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    model_state = {
                        'model_state_dict': net.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'loss': current_loss,
                        'iteration': iteration,
                        'mask_length': mask_length,
                        'mask_strategy': mask_strategy,
                        'format_info': format_info,
                    }
                    safe_save_model(model_state, checkpoint_dir / 'best_manual_masked_model.pt', "Best manual masked model")
                else:
                    no_improve_count += 1
                
                # æ£€æŸ¥æ—©æœŸåœæ­¢
                if no_improve_count >= patience:
                    print(f"ğŸ›‘ Early stopping at iteration {iteration}")
                    print(f"   No improvement for {patience} iterations")
                    print(f"   Best loss: {best_loss:.6f} at iteration {best_loss_iteration}")
                    break
                
                # æ‰“å°è¿›åº¦
                if iteration % 10 == 0:
                    current_lr = optimizer.param_groups[0]['lr']
                    print(f"Iter {iteration:4d}/{params.max_iter} | "
                          f"Loss: {current_loss:.6f} | "
                          f"Best: {best_loss:.6f} | "
                          f"NoImprove: {no_improve_count}/{patience} | "
                          f"LR: {current_lr:.2e}")
                
                # è¯¦ç»†ç›‘æ§
                if iteration % 50 == 0 and iteration > 0:
                    # è®¡ç®—æ¢¯åº¦èŒƒæ•°
                    total_norm = 0
                    for p in net.parameters():
                        if p.grad is not None:
                            param_norm = p.grad.data.norm(2)
                            total_norm += param_norm.item() ** 2
                    total_norm = total_norm ** (1. / 2)
                    
                    # è®¡ç®—å­¦ä¹ è¿›åº¦
                    if len(loss_history) >= 100:
                        recent_avg = sum(loss_history[-50:]) / 50
                        early_avg = sum(loss_history[:50]) / 50
                        improvement = (early_avg - recent_avg) / early_avg * 100
                        
                        print(f"  ğŸ“ˆ Grad norm: {total_norm:.4f} | "
                              f"Improvement: {improvement:.1f}% | "
                              f"Recent avg: {recent_avg:.6f}")
                
                # å®šæœŸä¿å­˜checkpoint
                if iteration > 0 and iteration % 1000 == 0:
                    checkpoint_path = checkpoint_dir / f"manual_masked_checkpoint_iter_{iteration}.pt"
                    checkpoint_state = {
                        'iteration': iteration,
                        'model_state_dict': net.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'loss': current_loss,
                        'loss_history': loss_history,
                        'best_loss': best_loss,
                        'mask_length': mask_length,
                        'mask_strategy': mask_strategy,
                        'format_info': format_info,
                        'no_improve_count': no_improve_count,
                    }
                    safe_save_model(checkpoint_state, checkpoint_path, f"Manual masked checkpoint {iteration}")
                
                iteration += 1
                
            except Exception as e:
                print(f"âŒ Error processing batch {batch_idx}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # å¦‚æœæ—©æœŸåœæ­¢ï¼Œè·³å‡ºå¤–å±‚å¾ªç¯
        if no_improve_count >= patience:
            break
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_state = {
        'model_state_dict': net.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'loss_history': loss_history,
        'best_loss': best_loss,
        'final_loss': loss_history[-1] if loss_history else float('inf'),
        'total_iterations': iteration,
        'mask_length': mask_length,
        'mask_strategy': mask_strategy,
        'format_info': format_info,
        'early_stopped': no_improve_count >= patience,
    }
    
    safe_save_model(final_model_state, checkpoint_dir / 'manual_masked_traffic_model_final.pt', "Final manual masked model")
    
    print(f"\nğŸ Manual masked training completed!")
    print(f"  â€¢ Total iterations: {iteration}")
    print(f"  â€¢ Best loss: {best_loss:.6f} (at iteration {best_loss_iteration})")
    print(f"  â€¢ Final loss: {loss_history[-1] if loss_history else 'N/A'}")
    print(f"  â€¢ Early stopped: {no_improve_count >= patience}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    if len(loss_history) > 10:
        try:
            plot_training_loss(loss_history, checkpoint_dir, prefix="manual_masked_")
        except Exception as e:
            print(f"âš ï¸ Could not save loss plot: {e}")

def train(params):
    """ä¸»è®­ç»ƒå‡½æ•°ï¼Œè‡ªåŠ¨é€‰æ‹©è®­ç»ƒæ–¹å¼"""
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨maskæ¨¡å¼
    use_mask = getattr(params, 'use_mask', False) or getattr(params, 'prediction_mode', False)
    
    if use_mask and MASK_MODEL_AVAILABLE:
        print("ğŸ­ Using mask-aware training mode with specialized trainer")
        train_with_mask(params)
    elif use_mask:
        print("ğŸ­ Using manual mask implementation")
        train_with_mask(params)
    else:
        print("ğŸ“Š Using standard training mode")
        train_original(params)

def train_original(params):
    """åŸå§‹è®­ç»ƒå‡½æ•°ï¼ˆä¿æŒä¸å˜ä½†æ·»åŠ äº†æ”¹è¿›ï¼‰"""
    torch.backends.cudnn.benchmark = True
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print(f"ğŸ–¥ï¸  Using device: {device}")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    model_dir = Path(params.model_dir)
    log_dir = Path(params.log_dir)
    model_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # å®šä¹‰checkpointç›®å½•å’Œæ–‡ä»¶è·¯å¾„
    checkpoint_dir = model_dir
    best_model_path = checkpoint_dir / 'best_model.pt'
    final_model_path = checkpoint_dir / 'traffic_model_final.pt'
    
    print(f"ğŸ“ Directories:")
    print(f"  â€¢ Model dir: {model_dir}")
    print(f"  â€¢ Log dir: {log_dir}")
    
    # åˆ›å»ºæ•°æ®é›†
    print("ğŸ“Š Creating dataset...")
    dataset = from_path(params)
    
    # éªŒè¯æ•°æ®é›†
    all_users, total_samples, batch_count = verify_dataset_split(dataset)
    
    if not all_users:
        print("âŒ è­¦å‘Š: æ— æ³•è·å–ç”¨æˆ·ä¿¡æ¯ï¼Œå¯èƒ½ä½¿ç”¨äº†é”™è¯¯çš„æ•°æ®é›†æ ¼å¼")
        print("   å°†ç»§ç»­è®­ç»ƒä½†æ— æ³•éªŒè¯ç”¨æˆ·åˆ’åˆ†...")
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ—ï¸ Creating model...")
    net = ConditionalUNet(params).to(device)
    print(f"ğŸ“Š Model parameters: {sum(p.numel() for p in net.parameters()):,}")
    
    # ä¼˜åŒ–å™¨è®¾ç½®
    optimizer = torch.optim.AdamW(
        net.parameters(), 
        lr=params.learning_rate, 
        weight_decay=1e-4
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, 
        T_max=params.max_iter, 
        eta_min=1e-7
    )
    
    # æŸå¤±å‡½æ•°
    mse_loss = torch.nn.MSELoss()
    l1_loss = torch.nn.L1Loss()
    
    def combined_loss(pred, target):
        mse = mse_loss(pred, target)
        l1 = l1_loss(pred, target)
        return mse + 0.1 * l1
    
    # è®­ç»ƒå˜é‡
    net.train()
    epoch = 0
    iteration = 0
    running_loss = 0.0
    best_loss = float('inf')
    loss_history = []
    
    # æ—©æœŸåœæ­¢
    patience = getattr(params, 'early_stopping_patience', 500)
    no_improve_count = 0
    
    print(f"ğŸš€ Starting training with enhanced settings...")
    print(f"ğŸ“‹ Training parameters:")
    print(f"  â€¢ Learning rate: {params.learning_rate}")
    print(f"  â€¢ Max iterations: {params.max_iter}")
    print(f"  â€¢ Batch size: {params.batch_size}")
    print(f"  â€¢ Dataset batches: {batch_count}")
    print(f"  â€¢ Total samples: {total_samples}")
    print(f"  â€¢ Early stopping patience: {patience}")
    
    # è®­ç»ƒå¾ªç¯
    while iteration < params.max_iter:
        epoch += 1
        print(f"\nğŸ“… Epoch {epoch} starting...")
        
        # é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨ç¡®ä¿æ¯ä¸ªepochçš„éšæœºæ€§
        try:
            data_loader = iter(dataset)
        except Exception as e:
            print(f"âŒ Error creating data loader: {e}")
            break
        
        epoch_loss = 0.0
        epoch_batches = 0
        
        # éå†å½“å‰epochçš„æ‰€æœ‰æ‰¹æ¬¡
        for batch_idx, batch in enumerate(data_loader):
            if iteration >= params.max_iter:
                print(f"ğŸ Reached maximum iterations ({params.max_iter})")
                break
            
            try:
                # éªŒè¯æ‰¹æ¬¡æ ¼å¼
                if not isinstance(batch, dict):
                    print(f"âš ï¸ Batch {batch_idx}: Not a dictionary, got {type(batch)}")
                    continue
                
                if 'data' not in batch:
                    print(f"âš ï¸ Batch {batch_idx}: Missing 'data' key, keys: {list(batch.keys())}")
                    continue
                
                # è·å–æ•°æ®
                data = batch['data'].to(device)
                cond = batch['cond'].to(device) if 'cond' in batch else None
                
                # éªŒè¯æ•°æ®å½¢çŠ¶
                if data.dim() != 4:
                    if data.dim() == 3:  # [B, T, F] -> [B, 1, T, F]
                        data = data.unsqueeze(1)
                    elif data.dim() == 2:  # [B, F] -> [B, 1, 1, F]
                        data = data.unsqueeze(1).unsqueeze(1)
                    else:
                        print(f"âš ï¸ Unexpected data shape: {data.shape}")
                        continue
                
                # é¦–æ¬¡è¿­ä»£æ—¶æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
                if iteration == 0:
                    print(f"\nğŸ“Š First batch statistics:")
                    print(f"  â€¢ Data shape: {data.shape}")
                    print(f"  â€¢ Data range: [{data.min().item():.4f}, {data.max().item():.4f}]")
                    print(f"  â€¢ Data mean: {data.mean().item():.4f}, std: {data.std().item():.4f}")
                    if cond is not None:
                        print(f"  â€¢ Cond shape: {cond.shape}")
                        print(f"  â€¢ Cond range: [{cond.min().item():.4f}, {cond.max().item():.4f}]")
                    print(f"  â€¢ Batch keys: {list(batch.keys())}")
                
                # æ‰©æ•£æ¨¡å‹è®­ç»ƒæ­¥éª¤
                batch_size = data.shape[0]
                
                # éšæœºæ—¶é—´æ­¥
                t = torch.randint(0, 1000, (batch_size,), device=device)
                
                # å™ªå£°è°ƒåº¦å‚æ•°
                beta_start, beta_end = 1e-4, 0.02
                betas = torch.linspace(beta_start, beta_end, 1000, device=device)
                alphas = 1. - betas
                alphas_cumprod = torch.cumprod(alphas, dim=0)
                
                # ç”Ÿæˆå™ªå£°
                noise = torch.randn_like(data)
                
                # è®¡ç®—å™ªå£°ç³»æ•°
                sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod[t])
                sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod[t])
                
                # é‡å¡‘ç³»æ•°ä»¥åŒ¹é…æ•°æ®ç»´åº¦
                shape = [-1] + [1] * (data.dim() - 1)
                sqrt_alphas_cumprod = sqrt_alphas_cumprod.view(shape)
                sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.view(shape)
                
                # æ·»åŠ å™ªå£°åˆ°æ•°æ®
                noisy_data = sqrt_alphas_cumprod * data + sqrt_one_minus_alphas_cumprod * noise
                
                # å‰å‘ä¼ æ’­
                optimizer.zero_grad()
                
                predicted_noise = net(noisy_data, t, cond)
                
                # è®¡ç®—æŸå¤±
                loss = combined_loss(predicted_noise, noise)
                
                # L2æ­£åˆ™åŒ–
                l2_reg = 0
                for param in net.parameters():
                    l2_reg += torch.norm(param, 2)
                loss += 1e-5 * l2_reg
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=2.0)
                
                optimizer.step()
                scheduler.step()
                
                # è®°å½•æŸå¤±
                current_loss = loss.item()
                running_loss += current_loss
                epoch_loss += current_loss
                loss_history.append(current_loss)
                epoch_batches += 1
                
                # æ—©æœŸåœæ­¢é€»è¾‘
                if current_loss < best_loss:
                    best_loss = current_loss
                    no_improve_count = 0
                    model_state = {
                        'model_state_dict': net.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'loss': current_loss,
                        'iteration': iteration,
                        'epoch': epoch,
                        'batch_idx': batch_idx,
                    }
                    safe_save_model(model_state, best_model_path, "Best model")
                else:
                    no_improve_count += 1
                
                # æ£€æŸ¥æ—©æœŸåœæ­¢
                if no_improve_count >= patience:
                    print(f"ğŸ›‘ Early stopping triggered at iteration {iteration}")
                    break
                
                # æ‰“å°è®­ç»ƒè¿›åº¦
                if iteration % 10 == 0:
                    avg_loss = running_loss / (iteration + 1)
                    current_lr = optimizer.param_groups[0]['lr']
                    
                    print(f"Iter {iteration:4d}/{params.max_iter} | "
                          f"Epoch {epoch:2d} Batch {batch_idx:2d} | "
                          f"Loss: {current_loss:.6f} | "
                          f"Best: {best_loss:.6f} | "
                          f"Avg: {avg_loss:.6f} | "
                          f"NoImprove: {no_improve_count}/{patience} | "
                          f"LR: {current_lr:.2e}")
                
                # è¯¦ç»†ç›‘æ§
                if iteration % 50 == 0 and iteration > 0:
                    # è®¡ç®—æ¢¯åº¦èŒƒæ•°
                    total_norm = 0
                    for p in net.parameters():
                        if p.grad is not None:
                            param_norm = p.grad.data.norm(2)
                            total_norm += param_norm.item() ** 2
                    total_norm = total_norm ** (1. / 2)
                    
                    pred_range = f"[{predicted_noise.min().item():.4f}, {predicted_noise.max().item():.4f}]"
                    noise_range = f"[{noise.min().item():.4f}, {noise.max().item():.4f}]"
                    
                    print(f"  ğŸ“ˆ Grad norm: {total_norm:.6f} | Pred: {pred_range} | True: {noise_range}")
                    
                    # å­¦ä¹ è¿›åº¦åˆ†æ
                    if iteration >= 100:
                        recent_avg = sum(loss_history[-50:]) / min(50, len(loss_history))
                        early_avg = sum(loss_history[:50]) / min(50, len(loss_history))
                        improvement = (early_avg - recent_avg) / early_avg * 100
                        print(f"  ğŸ“Š Improvement: {improvement:.2f}% (from {early_avg:.4f} to {recent_avg:.4f})")
                
                # å®šæœŸä¿å­˜checkpoint
                if iteration > 0 and iteration % 5000 == 0:
                    checkpoint_path = checkpoint_dir / f"checkpoint_iter_{iteration}.pt"
                    checkpoint_state = {
                        'iteration': iteration,
                        'epoch': epoch,
                        'batch_idx': batch_idx,
                        'model_state_dict': net.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'loss': current_loss,
                        'loss_history': loss_history,
                        'best_loss': best_loss,
                    }
                    safe_save_model(checkpoint_state, checkpoint_path, f"Checkpoint {iteration}")
                
                iteration += 1
                
            except Exception as e:
                print(f"âŒ Error processing batch {batch_idx} in epoch {epoch}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # å¦‚æœæ—©æœŸåœæ­¢ï¼Œè·³å‡ºå¤–å±‚å¾ªç¯
        if no_improve_count >= patience:
            break
        
        # Epochç»“æŸç»Ÿè®¡
        if epoch_batches > 0:
            epoch_avg_loss = epoch_loss / epoch_batches
            print(f"ğŸ“Š Epoch {epoch} completed:")
            print(f"  â€¢ Batches processed: {epoch_batches}")
            print(f"  â€¢ Average loss: {epoch_avg_loss:.6f}")
            print(f"  â€¢ Total iterations so far: {iteration}")
        else:
            print(f"âš ï¸ Epoch {epoch}: No valid batches processed")
            break
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"\nğŸ Training completed!")
    print(f"  â€¢ Total epochs: {epoch}")
    print(f"  â€¢ Total iterations: {iteration}")
    print(f"  â€¢ Best loss: {best_loss:.6f}")
    print(f"  â€¢ Early stopped: {no_improve_count >= patience}")
    
    final_model_state = {
        'model_state_dict': net.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'loss_history': loss_history,
        'best_loss': best_loss,
        'final_loss': loss_history[-1] if loss_history else float('inf'),
        'total_epochs': epoch,
        'total_iterations': iteration,
        'early_stopped': no_improve_count >= patience,
        'params': params.__dict__ if hasattr(params, '__dict__') else params,
    }
    
    if safe_save_model(final_model_state, final_model_path, "Final model"):
        print(f"ğŸ† Best loss achieved: {best_loss:.6f}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    if len(loss_history) > 10:
        try:
            plot_training_loss(loss_history, checkpoint_dir)
        except Exception as e:
            print(f"âš ï¸ Could not save loss plot: {e}")
    
    print("âœ… Training process completed successfully!")

def plot_training_loss(loss_history, save_dir, prefix=""):
    """ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿"""
    try:
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(15, 10))
        
        # åŸå§‹æŸå¤±
        plt.subplot(2, 3, 1)
        plt.plot(loss_history, alpha=0.7, linewidth=1)
        plt.title('Training Loss')
        plt.xlabel('Iteration')
        plt.ylabel('Loss')
        plt.grid(True, alpha=0.3)
        
        # å¯¹æ•°å°ºåº¦æŸå¤±
        plt.subplot(2, 3, 2)
        plt.plot(loss_history, alpha=0.7, linewidth=1)
        plt.title('Training Loss (Log Scale)')
        plt.xlabel('Iteration')
        plt.ylabel('Loss')
        plt.yscale('log')
        plt.grid(True, alpha=0.3)
        
        # æ»‘åŠ¨å¹³å‡
        if len(loss_history) > 50:
            window_size = min(50, len(loss_history) // 10)
            smoothed = []
            for i in range(window_size, len(loss_history)):
                smoothed.append(sum(loss_history[i-window_size:i]) / window_size)
            
            plt.subplot(2, 3, 3)
            plt.plot(range(window_size, len(loss_history)), smoothed, color='red', alpha=0.8, linewidth=2)
            plt.title(f'Smoothed Loss (window={window_size})')
            plt.xlabel('Iteration')
            plt.ylabel('Loss')
            plt.grid(True, alpha=0.3)
        
        # æœ€è¿‘çš„æŸå¤±
        if len(loss_history) > 100:
            plt.subplot(2, 3, 4)
            recent_loss = loss_history[-100:]
            plt.plot(range(len(loss_history)-100, len(loss_history)), recent_loss, color='green', alpha=0.8, linewidth=1.5)
            plt.title('Recent Loss (Last 100 iterations)')
            plt.xlabel('Iteration')
            plt.ylabel('Loss')
            plt.grid(True, alpha=0.3)
        
        # æŸå¤±å˜åŒ–ç‡
        if len(loss_history) > 10:
            loss_changes = [loss_history[i] - loss_history[i-1] for i in range(1, len(loss_history))]
            plt.subplot(2, 3, 5)
            plt.plot(loss_changes, alpha=0.6, color='purple', linewidth=1)
            plt.title('Loss Change Rate')
            plt.xlabel('Iteration')
            plt.ylabel('Loss Î”')
            plt.grid(True, alpha=0.3)
            plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)
        
        # ç»Ÿè®¡ä¿¡æ¯
        plt.subplot(2, 3, 6)
        stats_text = f'''Training Statistics

Total Iterations: {len(loss_history)}
Final Loss: {loss_history[-1]:.6f}
Best Loss: {min(loss_history):.6f}
Average Loss: {sum(loss_history)/len(loss_history):.6f}'''
        
        if len(loss_history) > 1:
            improvement = (loss_history[0] - loss_history[-1]) / loss_history[0] * 100
            stats_text += f'\nTotal Improvement: {improvement:.2f}%'
        
        plt.text(0.1, 0.9, stats_text, transform=plt.gca().transAxes, 
                fontsize=11, verticalalignment='top', fontfamily='monospace')
        plt.title('Training Statistics')
        plt.axis('off')
        
        plt.tight_layout()
        loss_plot_path = save_dir / f'{prefix}training_loss_analysis.png'
        plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š Training loss analysis saved: {loss_plot_path}")
        
    except ImportError:
        print("âš ï¸ matplotlib not available, skipping loss plot")
    except Exception as e:
        print(f"âš ï¸ Error creating loss plot: {e}")

def train_distributed(replica_id, replica_count, port, params):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = str(port)

    backend = 'nccl' if (torch.cuda.is_available() and os.name != 'nt') else 'gloo'

    if torch.cuda.is_available():
        torch.cuda.set_device(replica_id)

    torch.distributed.init_process_group(
        backend=backend, rank=replica_id, world_size=replica_count
    )

    dataset = from_path(params, is_distributed=True)

    device = torch.device(f'cuda:{replica_id}' if torch.cuda.is_available() else 'cpu')
    
    # æ ¹æ®æ˜¯å¦å¯ç”¨maské€‰æ‹©æ¨¡å‹
    use_mask = getattr(params, 'use_mask', False) or getattr(params, 'prediction_mode', False)
    if use_mask and MASK_MODEL_AVAILABLE:
        model = tfdiff_WiFi(params).to(device)
    else:
        model = tfdiff_WiFi(params).to(device)
    
    model = DistributedDataParallel(
        model,
        device_ids=[replica_id] if device.type == 'cuda' else None,
        output_device=replica_id if device.type == 'cuda' else None
    )
    _train_impl(replica_id, model, dataset, params)

def main(args):
    if args.task_id == 4:
        from params import AttrDict
        import numpy as np
        
        # ç¡®ä¿é¢„æµ‹é•¿åº¦åˆç†
        total_seq_len = 168
        pred_len = min(args.pred_len, total_seq_len // 2)  # æœ€å¤§ä¸è¶…è¿‡æ€»é•¿åº¦çš„ä¸€åŠ
        input_seq_len = total_seq_len - pred_len
        
        if input_seq_len < 10:  # ç¡®ä¿è¾“å…¥åºåˆ—è¶³å¤Ÿé•¿
            pred_len = total_seq_len - 10
            input_seq_len = 10
            print(f"âš ï¸ Adjusted pred_len to {pred_len} to ensure minimum input length")
        
        params = AttrDict(
            task_id=4,
            log_dir='./log/traffic_prediction',
            model_dir='./model/traffic_prediction',
            data_dir=['./dataset/traffic'],
            traffic_path='traffic_data_new.npz',
            embedding_path='environment_embeddings.npz',
            max_iter=args.max_iter or 50000,  # é»˜è®¤1000æ¬¡è¿­ä»£
            batch_size=args.batch_size or 64,
            learning_rate=1e-4,
            max_grad_norm=0.5,
            inference_batch_size=4,
            robust_sampling=True,
            
            # é¢„æµ‹ä»»åŠ¡ç›¸å…³å‚æ•°
            pred_len=pred_len,
            seq_len=input_seq_len,
            input_seq_len=input_seq_len,
            total_seq_len=total_seq_len,
            
            # æ¨¡å‹å‚æ•°
            input_dim=20,
            output_dim=20,
            extra_dim=[128],
            cond_dim=148,
            embed_dim=128,
            hidden_dim=128,
            num_heads=4,
            num_block=8,
            dropout=0.1,
            mlp_ratio=4.0,
            
            # æ‰©æ•£æ¨¡å‹å‚æ•°
            learn_tfdiff=False,
            max_step=1000,
            signal_diffusion=True,
            blur_schedule=((1e-5**2) * np.ones(1000)).tolist(),
            noise_schedule=np.linspace(1e-4, 0.02, 1000).tolist(),
            device=args.device or 'cuda',
            
            prediction_mode=True,
            sample_rate=20,
            
            # Maskç›¸å…³å‚æ•°ï¼ˆæ–°å¢ï¼‰
            use_mask=True,  # å¯ç”¨maskæ¨¡å¼
            mask_length=pred_len,  # maské•¿åº¦ç­‰äºé¢„æµ‹é•¿åº¦
            mask_strategy=getattr(args, 'mask_strategy', 'prefix'),  # maskç­–ç•¥
            mask_strategies=['prefix'],  # ä½¿ç”¨å‰ç¼€maskç­–ç•¥
            mask_weight=1.0,  # maskåŒºåŸŸæŸå¤±æƒé‡
            unmask_weight=0.1,  # émaskåŒºåŸŸæŸå¤±æƒé‡
            
            # æ—©æœŸåœæ­¢å‚æ•°
            early_stopping_patience=200,  # æ—©æœŸåœæ­¢çš„è€å¿ƒå€¼
        )
        
        print(f"ğŸ“Š Prediction Configuration:")
        print(f"  â€¢ Total sequence length: {total_seq_len}")
        print(f"  â€¢ Input sequence length: {input_seq_len}")
        print(f"  â€¢ Prediction length: {pred_len}")
        print(f"  â€¢ Mask length: {params.mask_length}")
        print(f"  â€¢ Mask strategy: {params.mask_strategy}")
        print(f"  â€¢ Use mask: {params.use_mask}")
        print(f"  â€¢ Early stopping patience: {params.early_stopping_patience}")
        print(f"  â€¢ Validation: {input_seq_len} + {pred_len} = {input_seq_len + pred_len} <= {total_seq_len}")
        
        # å‚æ•°éªŒè¯
        assert input_seq_len + pred_len == total_seq_len, f"åºåˆ—é•¿åº¦é…ç½®ä¸ä¸€è‡´: {input_seq_len} + {pred_len} != {total_seq_len}"
        assert params.mask_length <= pred_len, f"maské•¿åº¦({params.mask_length})ä¸èƒ½è¶…è¿‡é¢„æµ‹é•¿åº¦({pred_len})"
        assert input_seq_len >= 10, f"è¾“å…¥åºåˆ—é•¿åº¦({input_seq_len})è‡³å°‘éœ€è¦10ä¸ªæ—¶é—´æ­¥"
        
    else:
        params = all_params[args.task_id]
        if args.pred_len is not None:
            params.pred_len = args.pred_len
            params.prediction_mode = True
            # å¯ç”¨maskæ¨¡å¼
            params.use_mask = True
            params.mask_length = args.pred_len
            params.mask_strategy = getattr(args, 'mask_strategy', 'prefix')
            if hasattr(params, 'seq_len'):
                params.input_seq_len = params.seq_len - args.pred_len
                params.seq_len = params.input_seq_len
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.batch_size is not None:
        params.batch_size = args.batch_size
    if args.model_dir is not None:
        params.model_dir = args.model_dir
    if args.data_dir is not None:
        params.data_dir = args.data_dir
    if args.log_dir is not None:
        params.log_dir = args.log_dir
    if args.max_iter is not None:
        params.max_iter = args.max_iter
    
    # æ–°å¢maskç›¸å…³å‚æ•°å¤„ç†
    if hasattr(args, 'mask_length') and args.mask_length is not None:
        params.mask_length = args.mask_length
        params.use_mask = True
    
    if hasattr(args, 'mask_strategy') and args.mask_strategy is not None:
        params.mask_strategy = args.mask_strategy
    
    if hasattr(args, 'use_mask') and args.use_mask:
        params.use_mask = True
    
    # æ·»åŠ æ—©æœŸåœæ­¢å‚æ•°
    if not hasattr(params, 'early_stopping_patience'):
        params.early_stopping_patience = 200
    
    # éªŒè¯é¢„æµ‹é•¿åº¦å‚æ•°
    if hasattr(params, 'prediction_mode') and params.prediction_mode:
        if params.pred_len <= 0:
            raise ValueError(f"Prediction length must be positive, got {params.pred_len}")
        if hasattr(params, 'total_seq_len') and params.pred_len >= params.total_seq_len:
            raise ValueError(f"Prediction length ({params.pred_len}) must be less than total sequence length ({params.total_seq_len})")
        
        # éªŒè¯maské…ç½®
        if params.use_mask:
            if params.mask_length <= 0:
                raise ValueError(f"Mask length must be positive, got {params.mask_length}")
            if params.mask_length > params.pred_len:
                print(f"âš ï¸ Warning: mask_length ({params.mask_length}) > pred_len ({params.pred_len}), adjusting...")
                params.mask_length = params.pred_len
    
    print(f"\nğŸ¯ Final Configuration Summary:")
    print(f"  â€¢ Task ID: {params.task_id}")
    print(f"  â€¢ Max iterations: {params.max_iter}")
    print(f"  â€¢ Batch size: {params.batch_size}")
    print(f"  â€¢ Learning rate: {getattr(params, 'learning_rate', 'N/A')}")
    print(f"  â€¢ Use mask: {getattr(params, 'use_mask', False)}")
    if hasattr(params, 'use_mask') and params.use_mask:
        print(f"  â€¢ Mask length: {getattr(params, 'mask_length', 'N/A')}")
        print(f"  â€¢ Mask strategy: {getattr(params, 'mask_strategy', 'N/A')}")
    print(f"  â€¢ Early stopping patience: {getattr(params, 'early_stopping_patience', 'N/A')}")
    
    replica_count = device_count()
    
    if replica_count > 1 and getattr(params, 'device', 'cuda') != 'cpu':
        if params.batch_size % replica_count != 0:
            raise ValueError(
                f'Batch size {params.batch_size} is not evenly divisible by # GPUs {replica_count}.')
        params.batch_size = params.batch_size // replica_count
        port = _get_free_port()
        spawn(train_distributed, args=(replica_count, port, params), nprocs=replica_count, join=True)
    else:
        train(params)

if __name__ == '__main__':
    parser = ArgumentParser(
        description='train (or resume training) a tfdiff prediction model with mask support')
    parser.add_argument('--task_id', default=4, type=int,
                        help='use case of tfdiff model, 0/1/2/3/4 for WiFi/FMCW/MIMO/EEG/Traffic respectively')
    parser.add_argument('--pred_len', default=10, type=int,
                        help='prediction length (l parameter), number of time steps to predict')
    parser.add_argument('--model_dir', default=None,
                        help='directory in which to store model checkpoints and training logs')
    parser.add_argument('--data_dir', default=None, nargs='+',
                        help='space separated list of directories from which to read data files for training')
    parser.add_argument('--log_dir', default=None)
    parser.add_argument('--max_iter', default=50000, type=int,
                        help='maximum number of training iteration')
    parser.add_argument('--batch_size', default=None, type=int)
    parser.add_argument('--device', default=None, type=str,
                        help='device to use for training (cuda/cpu)')
    
    # æ–°å¢maskç›¸å…³å‚æ•°
    parser.add_argument('--mask_length', default=None, type=int,
                        help='length of mask (number of time steps to mask)')
    parser.add_argument('--use_mask', action='store_true',
                        help='enable mask-aware training')
    parser.add_argument('--mask_strategy', default='prefix', choices=['prefix', 'suffix', 'random'],
                        help='strategy for creating masks')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if args.pred_len <= 0:
        raise ValueError("--pred_len must be a positive integer")
    
    if args.mask_length is not None and args.mask_length <= 0:
        raise ValueError("--mask_length must be a positive integer")
    
    # å¦‚æœæŒ‡å®šäº†mask_lengthï¼Œè‡ªåŠ¨å¯ç”¨maskæ¨¡å¼
    if args.mask_length is not None:
        args.use_mask = True
    
    # å¦‚æœæŒ‡å®šäº†pred_lenï¼Œè‡ªåŠ¨å¯ç”¨maskæ¨¡å¼ï¼ˆé™¤éæ˜ç¡®ç¦ç”¨ï¼‰
    if args.pred_len is not None and not hasattr(args, 'no_mask'):
        args.use_mask = True
        if args.mask_length is None:
            args.mask_length = args.pred_len
    
    print(f"ğŸš€ Starting TFDiff training...")
    print(f"ğŸ“‹ Arguments: {vars(args)}")
    
    main(args)
