{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "398750ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.load('env_info.npz')\n",
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4404aecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 168, 17)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = data['e']\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fffd770",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = e[0,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b0a39aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52828168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting POI Text Generation and Encoding Pipeline\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Analyzing data distribution...:  17%|\u001b[35mâ–ˆâ–‹        \u001b[0m| 1/6 [00:00<00:00, 300.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaded successfully! Shape: (49, 168, 17)\n",
      "\n",
      "==================================================\n",
      "ğŸ“Š POI Data Analysis\n",
      "==================================================\n",
      "Shape: (49, 168, 17)\n",
      "Mean: 0.0000\n",
      "Std: 0.0000\n",
      "Min: 0.0000\n",
      "Max: 0.0000\n",
      "\n",
      "Per-category statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Medical care: mean=0.0000, std=0.0000\n",
      "  Hotel: mean=0.0000, std=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Business affairs: mean=0.0000, std=0.0000\n",
      "  Life service: mean=0.0000, std=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transportation hub: mean=0.0000, std=0.0000\n",
      "  Culture: mean=0.0000, std=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sports: mean=0.0000, std=0.0000\n",
      "  Residence: mean=0.0000, std=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Entertainment and leisure: mean=0.0000, std=0.0000\n",
      "  Scenic spot: mean=0.0000, std=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing categories:  59%|\u001b[36mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    \u001b[0m| 10/17 [00:01<00:00,  9.75it/s, Category=Scenic spot]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Government: mean=0.0000, std=0.0000\n",
      "  Factory: mean=0.0000, std=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shopping: mean=0.0000, std=0.0000\n",
      "  Restaurant: mean=0.0000, std=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing categories:  82%|\u001b[36mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– \u001b[0m| 14/17 [00:01<00:00,  9.73it/s, Category=Restaurant]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Education: mean=0.0000, std=0.0000\n",
      "  Landmark: mean=0.0000, std=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing categories: 100%|\u001b[36mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 17/17 [00:01<00:00,  9.72it/s, Category=Other]\n",
      "ğŸ”¤ Generating text descriptions...:  50%|\u001b[35mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     \u001b[0m| 3/6 [00:01<00:02,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Other: mean=0.0000, std=0.0000\n",
      "âœ… Text generator initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”¤ Generating text descriptions: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 8232/8232 [00:00<00:00, 27862.16points/s, User=49/49, Point=150/168]\n",
      "ğŸ¤– Initializing text encoder...:  67%|\u001b[35mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   \u001b[0m| 4/6 [00:02<00:00,  2.23it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated descriptions for 49 users!\n",
      "\n",
      "==================================================\n",
      "ğŸ“ Sample Descriptions\n",
      "==================================================\n",
      "\n",
      "ğŸ‘¤ User 1 (first 3 points):\n",
      "  ğŸ“ Point 1: This location has minimal POI activity.\n",
      "  ğŸ“ Point 2: This location has minimal POI activity.\n",
      "  ğŸ“ Point 3: This location has minimal POI activity.\n",
      "\n",
      "ğŸ‘¤ User 2 (first 3 points):\n",
      "  ğŸ“ Point 1: This location has minimal POI activity.\n",
      "  ğŸ“ Point 2: This location has minimal POI activity.\n",
      "  ğŸ“ Point 3: This location has minimal POI activity.\n",
      "ğŸ¤– Initializing BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc05be3246f7480b863c487b6df2e46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ea85dd707048d6a8e06fdc5fb80860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d729dbcd8c149fa9290aa42c3e0b055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cdd5b006fb48e3932d90862401cdb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c75ef375324fecb415d15a607fc5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model components: 100%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 2/2 [01:36<00:00, 48.16s/it, Component=BERT model loaded]\n",
      "ğŸ§  Encoding texts...:  83%|\u001b[35mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– \u001b[0m| 5/6 [01:38<00:28, 28.81s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Total descriptions to encode: 8232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ§  Encoding text batches: 100%|\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 515/515 [00:35<00:00, 14.47batch/s, Batch size=8, Progress=8232/8232 texts]\n",
      "ğŸ§  Encoding texts...: 100%|\u001b[35mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [02:13<00:00, 30.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final embeddings shape: torch.Size([49, 168, 128])\n",
      "\n",
      "==================================================\n",
      "ğŸ’¾ Saving Results\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving files: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 2/2 [00:00<00:00, 119.24it/s, File=poi_descriptions.json]\n",
      "ğŸ§  Encoding texts...: 100%|\u001b[35mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [02:13<00:00, 22.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ Pipeline completed successfully!\n",
      "============================================================\n",
      "ğŸ“ Generated files:\n",
      "  â€¢ poi_text_embeddings.npz - Contains embeddings and metadata\n",
      "  â€¢ poi_descriptions.json - Contains text descriptions\n",
      "ğŸ“Š Summary:\n",
      "  â€¢ Users processed: 49\n",
      "  â€¢ Sampling points per user: 168\n",
      "  â€¢ Total descriptions generated: 8232\n",
      "  â€¢ Embedding dimension: 128\n",
      "\n",
      "âœ… All tasks completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "class POITextGenerator:\n",
    "    def __init__(self):\n",
    "        self.poi_categories = [\n",
    "            'Medical care', 'Hotel', 'Business affairs', 'Life service', \n",
    "            'Transportation hub', 'Culture', 'Sports', 'Residence', \n",
    "            'Entertainment and leisure', 'Scenic spot', 'Government', \n",
    "            'Factory', 'Shopping', 'Restaurant', 'Education', 'Landmark', 'Other'\n",
    "        ]\n",
    "        \n",
    "        self.poi_descriptions = {\n",
    "            'Medical care': ['hospital', 'clinic', 'pharmacy', 'medical center', 'healthcare facility'],\n",
    "            'Hotel': ['hotel', 'accommodation', 'lodge', 'inn', 'resort'],\n",
    "            'Business affairs': ['office building', 'business center', 'corporate area', 'commercial district'],\n",
    "            'Life service': ['service center', 'utility office', 'community service', 'public service'],\n",
    "            'Transportation hub': ['station', 'terminal', 'transport hub', 'transit center', 'airport'],\n",
    "            'Culture': ['museum', 'gallery', 'cultural center', 'art venue', 'library'],\n",
    "            'Sports': ['gym', 'sports center', 'stadium', 'fitness facility', 'athletic venue'],\n",
    "            'Residence': ['residential area', 'housing complex', 'apartment building', 'neighborhood'],\n",
    "            'Entertainment and leisure': ['entertainment venue', 'leisure center', 'recreation area', 'amusement'],\n",
    "            'Scenic spot': ['tourist attraction', 'scenic area', 'landmark', 'viewpoint', 'park'],\n",
    "            'Government': ['government building', 'public office', 'administrative center', 'city hall'],\n",
    "            'Factory': ['industrial area', 'manufacturing plant', 'factory', 'production facility'],\n",
    "            'Shopping': ['shopping mall', 'retail store', 'market', 'shopping center', 'commercial area'],\n",
    "            'Restaurant': ['restaurant', 'dining area', 'food court', 'cafe', 'eatery'],\n",
    "            'Education': ['school', 'university', 'educational institution', 'campus', 'learning center'],\n",
    "            'Landmark': ['landmark', 'monument', 'historic site', 'notable building', 'famous location'],\n",
    "            'Other': ['mixed area', 'general facility', 'unspecified location', 'other venue']\n",
    "        }\n",
    "\n",
    "    def generate_text_description(self, poi_vector, sampling_point=None, user_id=None):\n",
    "        \"\"\"ä¸ºå•ä¸ªé‡‡æ ·ç‚¹çš„POIå‘é‡ç”Ÿæˆæ–‡æœ¬æè¿°\"\"\"\n",
    "        top_indices = np.argsort(poi_vector)[-5:][::-1]\n",
    "        top_values = poi_vector[top_indices]\n",
    "        \n",
    "        significant_pois = [(idx, val) for idx, val in zip(top_indices, top_values) if val > 0.1]\n",
    "        \n",
    "        if not significant_pois:\n",
    "            return \"This location has minimal POI activity.\"\n",
    "        \n",
    "        descriptions = []\n",
    "        for poi_idx, intensity in significant_pois[:3]:\n",
    "            category = self.poi_categories[poi_idx]\n",
    "            desc_words = self.poi_descriptions[category]\n",
    "            \n",
    "            if intensity > 0.8:\n",
    "                prefix = \"heavily concentrated with\"\n",
    "            elif intensity > 0.5:\n",
    "                prefix = \"moderately populated with\"\n",
    "            else:\n",
    "                prefix = \"has some\"\n",
    "            \n",
    "            desc = f\"{prefix} {desc_words[0]} facilities\"\n",
    "            descriptions.append(desc)\n",
    "        \n",
    "        if len(descriptions) == 1:\n",
    "            text = f\"This area {descriptions[0]}.\"\n",
    "        elif len(descriptions) == 2:\n",
    "            text = f\"This area {descriptions[0]} and {descriptions[1]}.\"\n",
    "        else:\n",
    "            text = f\"This area {', '.join(descriptions[:-1])}, and {descriptions[-1]}.\"\n",
    "            \n",
    "        return text\n",
    "\n",
    "    def generate_all_descriptions(self, data):\n",
    "        \"\"\"ä¸ºæ‰€æœ‰ç”¨æˆ·çš„æ‰€æœ‰é‡‡æ ·ç‚¹ç”Ÿæˆæ–‡æœ¬æè¿°\"\"\"\n",
    "        descriptions = []\n",
    "        total_points = data.shape[0] * data.shape[1]\n",
    "        \n",
    "        # åˆ›å»ºæ€»è¿›åº¦æ¡\n",
    "        with tqdm(total=total_points, desc=\"ğŸ”¤ Generating text descriptions\", \n",
    "                  unit=\"points\", colour=\"green\") as pbar:\n",
    "            \n",
    "            for user_id in range(data.shape[0]):\n",
    "                user_descriptions = []\n",
    "                \n",
    "                # ç”¨æˆ·çº§åˆ«çš„è¿›åº¦ä¿¡æ¯\n",
    "                pbar.set_postfix({\n",
    "                    'User': f\"{user_id + 1}/{data.shape[0]}\",\n",
    "                    'Current': f\"Processing user {user_id + 1}\"\n",
    "                })\n",
    "                \n",
    "                for sampling_point in range(data.shape[1]):\n",
    "                    poi_vector = data[user_id, sampling_point, :]\n",
    "                    text = self.generate_text_description(poi_vector, sampling_point, user_id)\n",
    "                    user_descriptions.append(text)\n",
    "                    \n",
    "                    # æ›´æ–°è¿›åº¦æ¡\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                    # æ¯50ä¸ªç‚¹æ˜¾ç¤ºä¸€æ¬¡è¯¦ç»†ä¿¡æ¯\n",
    "                    if (sampling_point + 1) % 50 == 0:\n",
    "                        pbar.set_postfix({\n",
    "                            'User': f\"{user_id + 1}/{data.shape[0]}\",\n",
    "                            'Point': f\"{sampling_point + 1}/{data.shape[1]}\"\n",
    "                        })\n",
    "                \n",
    "                descriptions.append(user_descriptions)\n",
    "                \n",
    "        return descriptions\n",
    "\n",
    "class POITextEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', hidden_dim=256, output_dim=128):\n",
    "        super(POITextEncoder, self).__init__()\n",
    "        \n",
    "        print(\"ğŸ¤– Initializing BERT model...\")\n",
    "        with tqdm(total=2, desc=\"Loading model components\", colour=\"blue\") as pbar:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'Component': 'Tokenizer loaded'})\n",
    "            \n",
    "            self.bert = BertModel.from_pretrained(model_name)\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'Component': 'BERT model loaded'})\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        # å†»ç»“BERTå‚æ•°\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        \"\"\"ç¼–ç æ–‡æœ¬åˆ—è¡¨ä¸ºembeddings\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        encoded = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(**encoded)\n",
    "            pooled_output = outputs.pooler_output\n",
    "        \n",
    "        embeddings = self.projection(pooled_output)\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def encode_batch(self, text_batch, batch_size=32):\n",
    "        \"\"\"æ‰¹é‡ç¼–ç æ–‡æœ¬\"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # åˆ›å»ºæ‰¹å¤„ç†è¿›åº¦æ¡\n",
    "        total_batches = (len(text_batch) + batch_size - 1) // batch_size\n",
    "        \n",
    "        with tqdm(total=total_batches, desc=\"ğŸ§  Encoding text batches\", \n",
    "                  unit=\"batch\", colour=\"yellow\") as pbar:\n",
    "            \n",
    "            for i in range(0, len(text_batch), batch_size):\n",
    "                batch_texts = text_batch[i:i+batch_size]\n",
    "                \n",
    "                # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡ä¿¡æ¯\n",
    "                pbar.set_postfix({\n",
    "                    'Batch size': len(batch_texts),\n",
    "                    'Progress': f\"{i + len(batch_texts)}/{len(text_batch)} texts\"\n",
    "                })\n",
    "                \n",
    "                embeddings = self.forward(batch_texts)\n",
    "                all_embeddings.append(embeddings.detach())\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # æ¨¡æ‹Ÿä¸€äº›å¤„ç†æ—¶é—´ï¼ˆå®é™…ä½¿ç”¨æ—¶å¯ä»¥åˆ é™¤ï¼‰\n",
    "                time.sleep(0.01)\n",
    "        \n",
    "        return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "def analyze_poi_distribution(poi_data):\n",
    "    \"\"\"åˆ†æPOIæ•°æ®åˆ†å¸ƒ\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“Š POI Data Analysis\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    poi_categories = [\n",
    "        'Medical care', 'Hotel', 'Business affairs', 'Life service', \n",
    "        'Transportation hub', 'Culture', 'Sports', 'Residence', \n",
    "        'Entertainment and leisure', 'Scenic spot', 'Government', \n",
    "        'Factory', 'Shopping', 'Restaurant', 'Education', 'Landmark', 'Other'\n",
    "    ]\n",
    "    \n",
    "    print(f\"Shape: {poi_data.shape}\")\n",
    "    print(f\"Mean: {poi_data.mean():.4f}\")\n",
    "    print(f\"Std: {poi_data.std():.4f}\")\n",
    "    print(f\"Min: {poi_data.min():.4f}\")\n",
    "    print(f\"Max: {poi_data.max():.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-category statistics:\")\n",
    "    with tqdm(poi_categories, desc=\"Analyzing categories\", colour=\"cyan\") as pbar:\n",
    "        for i, category in enumerate(pbar):\n",
    "            category_data = poi_data[:, :, i]\n",
    "            pbar.set_postfix({\n",
    "                'Category': category[:20] + \"...\" if len(category) > 20 else category\n",
    "            })\n",
    "            print(f\"  {category}: mean={category_data.mean():.4f}, std={category_data.std():.4f}\")\n",
    "            time.sleep(0.1)  # æ˜¾ç¤ºæ•ˆæœï¼Œå®é™…ä½¿ç”¨æ—¶å¯ä»¥åˆ é™¤\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "    print(\"ğŸš€ Starting POI Text Generation and Encoding Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # æ€»ä½“è¿›åº¦è·Ÿè¸ª\n",
    "    total_steps = 6\n",
    "    overall_progress = tqdm(total=total_steps, desc=\"ğŸ“‹ Overall Progress\", \n",
    "                          colour=\"magenta\", position=0)\n",
    "    \n",
    "    # æ­¥éª¤1: åŠ è½½æ•°æ®\n",
    "    overall_progress.set_description(\"ğŸ“ Loading NPZ file...\")\n",
    "    try:\n",
    "        data = np.load('env_info.npz')  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„\n",
    "        poi_data = data['e']\n",
    "        print(f\"âœ… Data loaded successfully! Shape: {poi_data.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®\n",
    "        print(\"âš ï¸  File not found, creating sample data...\")\n",
    "        poi_data = np.random.rand(49, 168, 17)\n",
    "        print(f\"âœ… Sample data created! Shape: {poi_data.shape}\")\n",
    "    \n",
    "    overall_progress.update(1)\n",
    "    \n",
    "    # æ­¥éª¤2: åˆ†ææ•°æ®åˆ†å¸ƒï¼ˆå¯é€‰ï¼‰\n",
    "    overall_progress.set_description(\"ğŸ“Š Analyzing data distribution...\")\n",
    "    analyze_poi_distribution(poi_data)\n",
    "    overall_progress.update(1)\n",
    "    \n",
    "    # æ­¥éª¤3: åˆ›å»ºæ–‡æœ¬ç”Ÿæˆå™¨\n",
    "    overall_progress.set_description(\"ğŸ”§ Initializing text generator...\")\n",
    "    text_generator = POITextGenerator()\n",
    "    print(\"âœ… Text generator initialized!\")\n",
    "    overall_progress.update(1)\n",
    "    \n",
    "    # æ­¥éª¤4: ç”Ÿæˆæ–‡æœ¬æè¿°\n",
    "    overall_progress.set_description(\"ğŸ”¤ Generating text descriptions...\")\n",
    "    all_descriptions = text_generator.generate_all_descriptions(poi_data)\n",
    "    print(f\"âœ… Generated descriptions for {len(all_descriptions)} users!\")\n",
    "    overall_progress.update(1)\n",
    "    \n",
    "    # æ˜¾ç¤ºæ ·æœ¬\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“ Sample Descriptions\")\n",
    "    print(\"=\"*50)\n",
    "    for i in range(min(2, len(all_descriptions))):\n",
    "        print(f\"\\nğŸ‘¤ User {i + 1} (first 3 points):\")\n",
    "        for j in range(min(3, len(all_descriptions[i]))):\n",
    "            print(f\"  ğŸ“ Point {j + 1}: {all_descriptions[i][j]}\")\n",
    "    \n",
    "    # æ­¥éª¤5: åˆå§‹åŒ–ç¼–ç å™¨\n",
    "    overall_progress.set_description(\"ğŸ¤– Initializing text encoder...\")\n",
    "    text_encoder = POITextEncoder(output_dim=128)\n",
    "    overall_progress.update(1)\n",
    "    \n",
    "    # æ­¥éª¤6: ç¼–ç æ–‡æœ¬\n",
    "    overall_progress.set_description(\"ğŸ§  Encoding texts...\")\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ®\n",
    "    flat_descriptions = []\n",
    "    for user_desc in all_descriptions:\n",
    "        flat_descriptions.extend(user_desc)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Total descriptions to encode: {len(flat_descriptions)}\")\n",
    "    \n",
    "    # æ‰¹é‡ç¼–ç \n",
    "    with torch.no_grad():\n",
    "        embeddings = text_encoder.encode_batch(flat_descriptions, batch_size=16)\n",
    "    \n",
    "    # é‡æ–°reshape\n",
    "    embeddings = embeddings.view(poi_data.shape[0], poi_data.shape[1], -1)\n",
    "    print(f\"âœ… Final embeddings shape: {embeddings.shape}\")\n",
    "    overall_progress.update(1)\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ’¾ Saving Results\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    save_tasks = [\"NPZ file\", \"JSON file\"]\n",
    "    with tqdm(save_tasks, desc=\"Saving files\", colour=\"green\") as save_pbar:\n",
    "        # ä¿å­˜NPZ\n",
    "        save_pbar.set_postfix({'File': 'poi_text_embeddings.npz'})\n",
    "        np.savez('poi_text_embeddings.npz', \n",
    "                 descriptions=np.array(all_descriptions, dtype=object),\n",
    "                 embeddings=embeddings.numpy(),\n",
    "                 categories=np.array(text_generator.poi_categories))\n",
    "        save_pbar.update(1)\n",
    "        \n",
    "        # ä¿å­˜JSON\n",
    "        save_pbar.set_postfix({'File': 'poi_descriptions.json'})\n",
    "        with open('poi_descriptions.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'descriptions': all_descriptions,\n",
    "                'categories': text_generator.poi_categories,\n",
    "                'metadata': {\n",
    "                    'total_users': poi_data.shape[0],\n",
    "                    'total_sampling_points': poi_data.shape[1],\n",
    "                    'poi_dimensions': poi_data.shape[2],\n",
    "                    'embedding_dimension': embeddings.shape[2]\n",
    "                }\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        save_pbar.update(1)\n",
    "    \n",
    "    overall_progress.close()\n",
    "    \n",
    "    print(\"\\nğŸ‰ Pipeline completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ“ Generated files:\")\n",
    "    print(\"  â€¢ poi_text_embeddings.npz - Contains embeddings and metadata\")\n",
    "    print(\"  â€¢ poi_descriptions.json - Contains text descriptions\")\n",
    "    print(f\"ğŸ“Š Summary:\")\n",
    "    print(f\"  â€¢ Users processed: {poi_data.shape[0]}\")\n",
    "    print(f\"  â€¢ Sampling points per user: {poi_data.shape[1]}\")\n",
    "    print(f\"  â€¢ Total descriptions generated: {len(flat_descriptions)}\")\n",
    "    print(f\"  â€¢ Embedding dimension: {embeddings.shape[2]}\")\n",
    "    \n",
    "    return text_generator, text_encoder, all_descriptions, embeddings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯é‡ç°æ€§\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # è¿è¡Œä¸»ç¨‹åº\n",
    "    try:\n",
    "        text_generator, text_encoder, descriptions, embeddings = main()\n",
    "        print(\"\\nâœ… All tasks completed successfully!\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâš ï¸  Process interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error occurred: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6989dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
