import os
import time
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from diffusion import SignalDiffusion, GaussianDiffusion
from dataset import _nested_map

device = torch.device('cuda')

import torch
import torch.nn as nn

class tfdiffLoss(nn.Module):
    def __init__(self, w=0.05, lambda_cosine=0.3):
        super().__init__()
        self.w = w
        self.lambda_cosine = lambda_cosine  # é‡å‘½åä¸ºæ›´å‡†ç¡®çš„å‚æ•°å
        self.eps = 1e-8

    def forward(self, target, est, target_noise=None, est_noise=None):
        # ç¡®ä¿è¾“å…¥æ˜¯æœ‰é™çš„
        if not torch.isfinite(target).all() or not torch.isfinite(est).all():
            print("âš ï¸ æŸå¤±å‡½æ•°è¾“å…¥åŒ…å«éæœ‰é™å€¼")
            return torch.tensor(0.0, device=target.device, requires_grad=True)
        
        # è®¡ç®—åŸºæœ¬çš„MSEæŸå¤±ï¼Œæ·»åŠ æ•°å€¼ç¨³å®šæ€§
        t_loss = self.stable_mse_loss(target, est)
        
        # è®¡ç®—å™ªå£°æŸå¤±
        if target_noise is not None and est_noise is not None:
            if torch.isfinite(target_noise).all() and torch.isfinite(est_noise).all():
                n_loss = self.stable_mse_loss(target_noise, est_noise)
            else:
                n_loss = 0.0
        else:
            n_loss = 0.0
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±
        try:
            cosine_loss = self.cosine_similarity_loss(target, est)
        except Exception as e:
            print(f"ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±è®¡ç®—å‡ºé”™: {e}")
            cosine_loss = 0.0
        
        # æ€»æŸå¤± - æ³¨æ„è¿™é‡Œä½¿ç”¨åŠ æ³•ï¼Œå› ä¸ºæˆ‘ä»¬è¦æœ€å°åŒ–(1-cosine_similarity)
        total_loss = t_loss + self.w * n_loss + self.lambda_cosine * cosine_loss
        
        # æœ€ç»ˆæ£€æŸ¥
        if not torch.isfinite(total_loss).all():
            print("âš ï¸ æ€»æŸå¤±åŒ…å«éæœ‰é™å€¼")
            return torch.tensor(0.0, device=target.device, requires_grad=True)
        
        # è£å‰ªæŸå¤±å€¼é¿å…è¿‡å¤§
        total_loss = torch.clamp(total_loss, 0.0, 1e6)
        
        return total_loss

    def stable_mse_loss(self, target, est):
        """æ•°å€¼ç¨³å®šçš„MSEæŸå¤±"""
        diff = target - est
        # è£å‰ªå·®å€¼ä»¥é¿å…è¿‡å¤§çš„å€¼
        diff = torch.clamp(diff, -1e3, 1e3)
        return torch.mean(diff ** 2)

    def cosine_similarity_loss(self, target, est):
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±
        
        Args:
            target: çœŸå®å€¼çŸ©é˜µ X_true âˆˆ R^{TÃ—C}
            est: é¢„æµ‹å€¼çŸ©é˜µ XÌ‚ âˆˆ R^{TÃ—C}
            
        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤± (1 - cosine_similarity)
        """
        batch_size = target.shape[0]
        cosine_losses = []
        
        for i in range(batch_size):
            # è·å–å•ä¸ªæ ·æœ¬çš„ç›®æ ‡å’Œé¢„æµ‹çŸ©é˜µ
            target_sample = target[i]  # shape: [T, C] æˆ–å…¶ä»–ç»´åº¦
            est_sample = est[i]        # shape: [T, C] æˆ–å…¶ä»–ç»´åº¦
            
            # å°†çŸ©é˜µå±•å¹³ä¸ºå‘é‡ vec(X)
            a = target_sample.flatten()  # vec(X_true)
            b = est_sample.flatten()     # vec(XÌ‚)
            
            # è®¡ç®—å‘é‡çš„L2èŒƒæ•°
            norm_a = torch.sqrt(torch.sum(a**2) + self.eps)
            norm_b = torch.sqrt(torch.sum(b**2) + self.eps)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦: cos(Î¸) = (aÂ·b) / (||a|| * ||b||)
            dot_product = torch.sum(a * b)
            cosine_similarity = dot_product / (norm_a * norm_b + self.eps)
            
            # è£å‰ªä½™å¼¦ç›¸ä¼¼åº¦åˆ°åˆç†èŒƒå›´ [-1, 1]
            cosine_similarity = torch.clamp(cosine_similarity, -1.0, 1.0)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±: 1 - cos(Î¸)
            # å½“å‘é‡å®Œå…¨ç›¸ä¼¼æ—¶ï¼Œcos(Î¸) = 1ï¼ŒæŸå¤± = 0
            # å½“å‘é‡å®Œå…¨ä¸ç›¸ä¼¼æ—¶ï¼Œcos(Î¸) = -1ï¼ŒæŸå¤± = 2
            cosine_loss = 1.0 - cosine_similarity
            cosine_losses.append(cosine_loss)
        
        # è¿”å›æ‰¹æ¬¡çš„å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±
        return torch.stack(cosine_losses).mean()

    def stable_cross_loss(self, target, est):
        """
        ä¿ç•™åŸå§‹çš„äº¤å‰æŸå¤±å‡½æ•°ä»¥å¤‡éœ€è¦æ—¶ä½¿ç”¨
        """
        batch_size, num_channels = target.shape[0], target.shape[-1]
        
        try:
            target = target.reshape(batch_size, -1, num_channels)
            est = est.reshape(batch_size, -1, num_channels)
            
            cross_corr = torch.mean(target * est, dim=1)
            target_norm = torch.sqrt(torch.mean(target**2, dim=1) + self.eps)
            est_norm = torch.sqrt(torch.mean(est**2, dim=1) + self.eps)
            
            normalized_cross_corr = cross_corr / (target_norm * est_norm + self.eps)
            # è£å‰ªç»“æœé¿å…æå€¼
            normalized_cross_corr = torch.clamp(normalized_cross_corr, -1.0, 1.0)
            
            return torch.mean(normalized_cross_corr)
        except Exception as e:
            print(f"äº¤å‰æŸå¤±è®¡ç®—å‡ºé”™: {e}")
            return torch.tensor(0.0, device=target.device)


class tfdiffLearner:
    def __init__(self, log_dir, model_dir, model, dataset, optimizer, params, *args, **kwargs):
        os.makedirs(model_dir, exist_ok=True)
         
        self.model_dir = model_dir
        self.task_id = params.task_id
        self.log_dir = log_dir
        self.model = model
        self.dataset = dataset
        self.optimizer = optimizer
        self.device = next(self.model.parameters()).device
        self.diffusion = SignalDiffusion(params) if params.signal_diffusion else GaussianDiffusion(params)
        self.diffusion.to(self.device)   # â† å…³é”®
        self.lr_scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1, gamma=0.99)  # æ›´æ¸©å’Œçš„å­¦ä¹ ç‡è¡°å‡
        self.params = params
        self.iter = 0
        self.is_master = True
        self.loss_fn = tfdiffLoss()
        self.summary_writer = None
        self.skip_count = 0  # è·³è¿‡çš„æ‰¹æ¬¡è®¡æ•°
        self.max_skip_ratio = 0.5  # æœ€å¤§è·³è¿‡æ¯”ä¾‹
        # ä¸ºæœ¬æ¬¡è¿è¡Œåˆ›å»ºå”¯ä¸€å­ç›®å½•
        self.run_id = f"run_{int(time.time())}_{os.getpid()}"
        self.run_log_dir = os.path.join(self.log_dir, self.run_id)
        os.makedirs(self.run_log_dir, exist_ok=True)
        # åˆå§‹åŒ–æ¨¡å‹æƒé‡
        self._initialize_model_weights()

    def _initialize_model_weights(self):
        """å®‰å…¨çš„æ¨¡å‹æƒé‡åˆå§‹åŒ–"""
        print("ğŸ”§ åˆå§‹åŒ–æ¨¡å‹æƒé‡...")
        
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):
                # Xavieræ­£æ€åˆ†å¸ƒåˆå§‹åŒ–ï¼Œä½¿ç”¨æ›´å°çš„å¢ç›Š
                nn.init.xavier_normal_(module.weight, gain=0.01)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, (nn.Conv1d, nn.Conv2d)):
                # Kaimingæ­£æ€åˆ†å¸ƒåˆå§‹åŒ–ï¼Œä½¿ç”¨æ›´å°çš„å¢ç›Š
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if hasattr(module.weight, 'data'):
                    module.weight.data *= 0.1  # è¿›ä¸€æ­¥ç¼©å°åˆå§‹æƒé‡
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d, nn.BatchNorm2d)):
                if module.weight is not None:
                    nn.init.constant_(module.weight, 1)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=0.01)
        
        print("âœ… æ¨¡å‹æƒé‡åˆå§‹åŒ–å®Œæˆ")

    def state_dict(self):
        if hasattr(self.model, 'module') and isinstance(self.model.module, nn.Module):
            model_state = self.model.module.state_dict()
        else:
            model_state = self.model.state_dict()
        return {
            'iter': self.iter,
            'model': {k: v.cpu() if isinstance(v, torch.Tensor) else v for k, v in model_state.items()},
            'optimizer': {k: v.cpu() if isinstance(v, torch.Tensor) else v for k, v in self.optimizer.state_dict().items()},
            'params': dict(self.params),
        }

    def load_state_dict(self, state_dict):
        if hasattr(self.model, 'module') and isinstance(self.model.module, nn.Module):
            self.model.module.load_state_dict(state_dict['model'])
        else:
            self.model.load_state_dict(state_dict['model'])
        self.optimizer.load_state_dict(state_dict['optimizer'])
        self.iter = state_dict['iter']

    def save_to_checkpoint(self, filename='weights'):
        timestamp = time.strftime('%Y%m%d-%H%M%S')
        save_basename = f'{filename}-{timestamp}.pt'
        save_name = os.path.join(self.model_dir, save_basename)
        link_name = os.path.join(self.model_dir, f'{filename}.pt')

        # ç¡®ä¿ä¿å­˜è·¯å¾„å­˜åœ¨
        os.makedirs(self.model_dir, exist_ok=True)

        # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
        torch.save(self.state_dict(), save_name)
        if os.name == 'nt':
            torch.save(self.state_dict(), link_name)
        else:
            # å¦‚æœå­˜åœ¨è½¯é“¾æ¥ï¼Œåˆ é™¤æ—§é“¾æ¥å¹¶æ›´æ–°
            if os.path.exists(link_name):
                os.remove(link_name)
            os.rename(save_name, link_name)

    def restore_from_checkpoint(self, filename='weights', replica_id=None):
        checkpoint_path = os.path.join(self.model_dir, f'{filename}.pt')
        try:
            # å¦‚æœä¸å­˜åœ¨æ–‡ä»¶ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            if not os.path.exists(checkpoint_path):
                print(f"No checkpoint found at {checkpoint_path}, starting fresh training")
                return False
            
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.load_state_dict(checkpoint)
            print(f"âœ… Restored from checkpoint at iteration {self.iter}")
            return True
        except Exception as e:
            print(f"Error loading checkpoint: {e}")
            return False

    def _stable_normalize(self, data, eps=1e-8):
        """æ•°å€¼ç¨³å®šçš„å½’ä¸€åŒ–"""
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        dims = tuple(range(1, data.ndim))
        mean = data.mean(dim=dims, keepdim=True)
        std = data.std(dim=dims, keepdim=True)
        
        # é¿å…é™¤é›¶
        std = torch.clamp(std, min=eps)
        
        # å½’ä¸€åŒ–
        normalized = (data - mean) / std
        
        # è£å‰ªæå€¼
        normalized = torch.clamp(normalized, -5.0, 5.0)
        
        return normalized
    
    def _reinitialize_parameter(self, param):
        """é‡æ–°åˆå§‹åŒ–å•ä¸ªå‚æ•°"""
        with torch.no_grad():
            if param.dim() == 1:
                nn.init.constant_(param, 0)
            else:
                nn.init.xavier_normal_(param, gain=0.01)
    
    def _check_and_fix_gradients(self):
        """æ£€æŸ¥å’Œä¿®å¤æ¢¯åº¦é—®é¢˜"""
        has_valid_grad = False
        total_norm = 0.0
        
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰é™
                if not torch.isfinite(param.grad).all():
                    print(f"âš ï¸ å‚æ•° {name} çš„æ¢¯åº¦åŒ…å«éæœ‰é™å€¼ï¼Œæ¸…é›¶æ¢¯åº¦")
                    param.grad.zero_()
                    continue
                
                # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦è¿‡å¤§
                grad_norm = param.grad.data.norm(2).item()
                if grad_norm > 1e2:
                    print(f"âš ï¸ å‚æ•° {name} çš„æ¢¯åº¦è¿‡å¤§ ({grad_norm:.2e})ï¼Œè¿›è¡Œè£å‰ª")
                    param.grad.data = torch.clamp(param.grad.data, -1e2, 1e2)
                    grad_norm = param.grad.data.norm(2).item()
                
                total_norm += grad_norm ** 2
                has_valid_grad = True
        
        if not has_valid_grad:
            print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ¢¯åº¦ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
            return False
        
        total_norm = total_norm ** 0.5
        if total_norm == 0:
            print("âš ï¸ æ€»æ¢¯åº¦èŒƒæ•°ä¸ºé›¶ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
            return False
        
        return True

    def train_iter(self, features):
        """æ”¹è¿›çš„è®­ç»ƒè¿­ä»£å‡½æ•°ï¼Œå¤„ç†æ¢¯åº¦æ•°å€¼ä¸ç¨³å®šé—®é¢˜"""
        
        # æ¸…é›¶æ¢¯åº¦
        self.optimizer.zero_grad()
        
        try:
            data = features['data']
            cond = features['cond']
        except KeyError as e:
            print(f"âŒ Missing key in features: {e}")
            return None
        
        # 1. æ›´ä¸¥æ ¼çš„æ•°æ®æ£€æŸ¥
        if not torch.isfinite(data).all():
            print("âš ï¸ è¾“å…¥æ•°æ®åŒ…å«éæœ‰é™å€¼ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡ã€‚")
            self.skip_count += 1
            return None
        
        if not torch.isfinite(cond).all():
            print("âš ï¸ æ¡ä»¶æ•°æ®åŒ…å«éæœ‰é™å€¼ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡ã€‚")
            self.skip_count += 1
            return None
        
        # 2. æ•°æ®èŒƒå›´æ£€æŸ¥å’Œè£å‰ª
        if data.abs().max() > 1e3:
            print(f"âš ï¸ è¾“å…¥æ•°æ®å€¼è¿‡å¤§ (max: {data.abs().max():.2e})ï¼Œè¿›è¡Œè£å‰ª")
            data = torch.clamp(data, -1e3, 1e3)
        
        if cond.abs().max() > 1e3:
            print(f"âš ï¸ æ¡ä»¶æ•°æ®å€¼è¿‡å¤§ (max: {cond.abs().max():.2e})ï¼Œè¿›è¡Œè£å‰ª")
            cond = torch.clamp(cond, -1e3, 1e3)
        
        # 3. æ•°æ®å½’ä¸€åŒ–ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
        data = self._stable_normalize(data)
        cond = self._stable_normalize(cond)
        
        # 4. æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦æœ‰é—®é¢˜
        for name, param in self.model.named_parameters():
            if not torch.isfinite(param).all():
                print(f"âŒ æ¨¡å‹å‚æ•° {name} åŒ…å«éæœ‰é™å€¼! é‡æ–°åˆå§‹åŒ–...")
                self._reinitialize_parameter(param)
                return None
        
        B = data.shape[0]
        t = torch.randint(0, self.diffusion.max_step, [B], dtype=torch.int64, device=data.device)
        
        # 5. æ‰©æ•£è¿‡ç¨‹
        try:
            degrade_data = self.diffusion.degrade_fn(data, t, self.task_id)
        except Exception as e:
            print(f"âŒ æ‰©æ•£è¿‡ç¨‹å‡ºé”™: {e}")
            self.skip_count += 1
            return None
        
        # æ£€æŸ¥æ‰©æ•£åçš„æ•°æ®
        if not torch.isfinite(degrade_data).all():
            print("âš ï¸ æ‰©æ•£åçš„æ•°æ®åŒ…å«éæœ‰é™å€¼ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡ã€‚")
            self.skip_count += 1
            return None
        
        # 6. æ¨¡å‹å‰å‘ä¼ æ’­
        try:
            predicted = self.model(degrade_data, t, cond)
        except Exception as e:
            print(f"âŒ æ¨¡å‹å‰å‘ä¼ æ’­å‡ºé”™: {e}")
            self.skip_count += 1
            return None
        
        # æ£€æŸ¥é¢„æµ‹ç»“æœ
        if not torch.isfinite(predicted).all():
            print("âš ï¸ é¢„æµ‹ç»“æœåŒ…å«éæœ‰é™å€¼ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡ã€‚")
            self.skip_count += 1
            return None
        
        # 7. è®¡ç®—æŸå¤±
        try:
            loss = self.loss_fn(data, predicted)
        except Exception as e:
            print(f"âŒ æŸå¤±è®¡ç®—å‡ºé”™: {e}")
            self.skip_count += 1
            return None
        
        # æ£€æŸ¥æŸå¤±å€¼
        if not torch.isfinite(loss).all():
            print(f"âš ï¸ æŸå¤±å€¼å¼‚å¸¸ ({loss.item()})ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡ã€‚")
            self.skip_count += 1
            return None
        
        # æ£€æŸ¥æŸå¤±å€¼æ˜¯å¦è¿‡å¤§
        if loss.item() > 1e3:
            print(f"âš ï¸ æŸå¤±å€¼è¿‡å¤§ ({loss.item():.2e})ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡ã€‚")
            self.skip_count += 1
            return None
        
        # 8. åå‘ä¼ æ’­
        try:
            loss.backward()
        except Exception as e:
            print(f"âŒ åå‘ä¼ æ’­å‡ºé”™: {e}")
            self.skip_count += 1
            return None
        
        # 9. æ£€æŸ¥å’Œå¤„ç†æ¢¯åº¦
        if not self._check_and_fix_gradients():
            self.skip_count += 1
            return None
        
        # 10. æ¢¯åº¦è£å‰ª - ä½¿ç”¨æ›´å°çš„é˜ˆå€¼
        max_grad_norm = getattr(self.params, 'max_grad_norm', 0.5)  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
        self.grad_norm = nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)
        
        # 11. ä¼˜åŒ–å™¨æ­¥éª¤
        try:
            self.optimizer.step()
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–å™¨æ­¥éª¤å‡ºé”™: {e}")
            self.skip_count += 1
            return None
        
        # 12. åå¤„ç†æ£€æŸ¥
        for name, param in self.model.named_parameters():
            if not torch.isfinite(param).all():
                print(f"âŒ ä¼˜åŒ–åå‚æ•° {name} åŒ…å«éæœ‰é™å€¼! é‡æ–°åˆå§‹åŒ–...")
                self._reinitialize_parameter(param)
                return None
        
        return loss

    def train(self, max_iter=None):
        device = next(self.model.parameters()).device
        epoch = 0
        total_processed_batches = 0
        
        print(f"ğŸš€ Starting training with max_iter={max_iter}")
        print(f"ğŸ“Š Dataset length: {len(self.dataset)}")
        try:
            while True:  # epoch loop
                epoch_start_iter = self.iter
                epoch_processed_batches = 0
                epoch_skipped_batches = 0
                print(f"\nğŸ”„ Starting Epoch {epoch + 1}")
                
                # é‡ç½®è·³è¿‡è®¡æ•°
                self.skip_count = 0
                
                # åˆ›å»ºè¿›åº¦æ¡
                pbar = tqdm(self.dataset, desc=f'Epoch {epoch + 1}', disable=not self.is_master)
                
                for batch_idx, features in enumerate(pbar):
                    if max_iter is not None and self.iter >= max_iter:
                        print(f"âœ… Reached max_iter {max_iter}, stopping training")
                        pbar.close()
                        return
                    
                    # ç¡®ä¿æ•°æ®ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                    # train() å†… for å¾ªç¯é‡Œ
                    features = _nested_map(features, lambda x: x.to(device, non_blocking=True) if isinstance(x, torch.Tensor) else x)

                    loss = self.train_iter(features)
                    
                    if loss is None:
                        epoch_skipped_batches += 1
                        # æ£€æŸ¥è·³è¿‡æ¯”ä¾‹æ˜¯å¦è¿‡é«˜
                        if epoch_processed_batches > 0:
                            skip_ratio = epoch_skipped_batches / (epoch_processed_batches + epoch_skipped_batches)
                            if skip_ratio > self.max_skip_ratio:
                                print(f"âŒ è·³è¿‡æ¯”ä¾‹è¿‡é«˜ ({skip_ratio:.2%})ï¼Œå¯èƒ½å­˜åœ¨ä¸¥é‡é—®é¢˜")
                                print("   å»ºè®®æ£€æŸ¥:")
                                print("   1. æ•°æ®é¢„å¤„ç†æ˜¯å¦æ­£ç¡®")
                                print("   2. æ¨¡å‹æ¶æ„æ˜¯å¦åˆé€‚")
                                print("   3. å­¦ä¹ ç‡æ˜¯å¦è¿‡å¤§")
                                print("   4. åˆå§‹åŒ–æ˜¯å¦åˆç†")
                                # é™ä½å­¦ä¹ ç‡å°è¯•æ¢å¤
                                for param_group in self.optimizer.param_groups:
                                    param_group['lr'] *= 0.5
                                print(f"   å·²å°†å­¦ä¹ ç‡é™ä½åˆ° {self.optimizer.param_groups[0]['lr']:.2e}")
                        continue

                    # æ£€æŸ¥æŸå¤±æ˜¯å¦åˆç†
                    loss_val = loss.item()
                    if loss_val > 1e6:
                        print(f"âš ï¸ æŸå¤±å€¼å¼‚å¸¸å¤§: {loss_val:.2e}")
                    elif loss_val < 1e-10:
                        print(f"âš ï¸ æŸå¤±å€¼å¼‚å¸¸å°: {loss_val:.2e}")

                    # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                    if self.is_master:
                        pbar.set_postfix({
                            'loss': f'{loss_val:.6f}',
                            'iter': self.iter,
                            'batch': epoch_processed_batches + 1,
                            'skipped': epoch_skipped_batches,
                            'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
                        })
                        
                        if self.iter % 50 == 0:
                            self._write_summary(self.iter, features, loss)
                            
                    epoch_processed_batches += 1
                    total_processed_batches += 1
                    self.iter += 1
                
                pbar.close()
                
                # Epoch ç»“æŸåçš„å¤„ç†
                epoch += 1
                skip_ratio = epoch_skipped_batches / max(1, epoch_processed_batches + epoch_skipped_batches)
                
                print(f"âœ… Completed Epoch {epoch}")
                print(f"ğŸ“ˆ Processed batches: {epoch_processed_batches}")
                print(f"âš ï¸ Skipped batches: {epoch_skipped_batches} ({skip_ratio:.1%})")
                print(f"ğŸ“Š Total processed batches so far: {total_processed_batches}")
                
                # å­¦ä¹ ç‡è°ƒåº¦
                old_lr = self.optimizer.param_groups[0]['lr']
                self.lr_scheduler.step()
                new_lr = self.optimizer.param_groups[0]['lr']
                if old_lr != new_lr:
                    print(f"ğŸ“‰ Learning rate updated: {old_lr:.2e} -> {new_lr:.2e}")
                
                # æ¯ä¸ª epoch ç»“æŸåä¿å­˜æ£€æŸ¥ç‚¹
                if self.is_master:
                    self.save_to_checkpoint()
                    print(f"ğŸ’¾ Checkpoint saved at iteration {self.iter}")
                    
                # å¦‚æœè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé€€å‡º
                if max_iter is not None and self.iter >= max_iter:
                    print(f"ğŸ‰ Training completed after {epoch} epochs and {self.iter} iterations")
                    return
                
                # æ£€æŸ¥æ˜¯å¦å¤„ç†äº†è¶³å¤Ÿçš„æ‰¹æ¬¡
                if epoch_processed_batches == 0:
                    print("âŒ Warning: No batches were processed in this epoch!")
                    print("   This might indicate a serious problem with the dataset or model.")
                    print("   å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ:")
                    print("   1. æ£€æŸ¥æ•°æ®èŒƒå›´å’Œåˆ†å¸ƒ")
                    print("   2. é™ä½å­¦ä¹ ç‡")
                    print("   3. é‡æ–°åˆå§‹åŒ–æ¨¡å‹")
                    
                    # å°è¯•æ¢å¤æªæ–½
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] *= 0.1
                    print(f"   å­¦ä¹ ç‡å·²é™ä½åˆ° {self.optimizer.param_groups[0]['lr']:.2e}")
                    
                    # é‡æ–°åˆå§‹åŒ–æ¨¡å‹æƒé‡
                    self._initialize_model_weights()
                    print("   æ¨¡å‹æƒé‡å·²é‡æ–°åˆå§‹åŒ–")
                    
                    if epoch > 3:  # å¦‚æœè¿ç»­å¤šä¸ªepochéƒ½æ— æ³•å¤„ç†ï¼Œåˆ™åœæ­¢
                        print("âŒ è¿ç»­å¤šä¸ªepochæ— æ³•å¤„ç†æ‰¹æ¬¡ï¼Œåœæ­¢è®­ç»ƒ")
                        break
        finally:
            if self.summary_writer is not None:
                try:
                    self.summary_writer.flush()
                    self.summary_writer.close()
                except Exception:
                    pass

    def _write_summary(self, iter, features, loss):
        # åªåˆå§‹åŒ–ä¸€æ¬¡ï¼›æŒ‡å®šå”¯ä¸€åç¼€ï¼Œå‡å°‘å†²çªï¼›é€‚åº¦é˜Ÿåˆ—+è‡ªåŠ¨åˆ·ç›˜
        if self.summary_writer is None:
            self.summary_writer = SummaryWriter(
                self.run_log_dir,
                purge_step=iter,
                filename_suffix=f".{os.getpid()}",
                max_queue=50,
                flush_secs=10,
            )

        try:
            loss_scalar = loss.item() if torch.is_tensor(loss) else float(loss)
            self.summary_writer.add_scalar('train/loss', loss_scalar, iter)
            if hasattr(self, "grad_norm"):
                gn = self.grad_norm.item() if torch.is_tensor(self.grad_norm) else float(self.grad_norm)
                self.summary_writer.add_scalar('train/grad_norm', gn, iter)
            self.summary_writer.add_scalar('train/learning_rate', self.optimizer.param_groups[0]['lr'], iter)
            self.summary_writer.flush()
        except PermissionError as e:
            print(f"[TensorBoard] å†™å…¥å¤±è´¥ï¼š{e}ï¼Œåˆ‡æ¢æ–°æ—¥å¿—ç›®å½•ç»§ç»­å†™å…¥ã€‚")
            # å…œåº•ï¼šæ¢æ–°ç›®å½•é‡å¼€ writer
            try:
                self.summary_writer.close()
            except Exception:
                pass
            reopen = os.path.join(self.log_dir, f"reopen_{int(time.time())}_{os.getpid()}")
            os.makedirs(reopen, exist_ok=True)
            self.summary_writer = SummaryWriter(reopen, purge_step=iter, filename_suffix=f".{os.getpid()}")
