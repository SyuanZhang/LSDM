import numpy as np
import os
import random
import torch
import torch.nn.functional as F
import scipy.io as scio
from params import AttrDict
from glob import glob
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from pathlib import Path


class TrafficPredictionDataset(Dataset):
    def __init__(self, traffic_path="traffic_data.npz", embedding_path="environment_embeddings.npz",
                 mode='train', train_ratio=0.6, val_ratio=0.2, test_ratio=0.2,
                 hist_len=148, pred_len=20, stride=1):
        """
        æµé‡é¢„æµ‹æ•°æ®é›† - æŒ‰ç”¨æˆ·åˆ’åˆ†ç‰ˆæœ¬
        """
        super().__init__()
        
        self.mode = mode
        self.hist_len = hist_len
        self.pred_len = pred_len
        self.stride = stride
        
        print(f"ğŸ—ï¸ åˆå§‹åŒ–æµé‡é¢„æµ‹æ•°æ®é›† ({mode} æ¨¡å¼)")
        print(f"ğŸ“‹ é…ç½®:")
        print(f"  â€¢ å†å²é•¿åº¦: {hist_len}")
        print(f"  â€¢ é¢„æµ‹é•¿åº¦: {pred_len}")
        print(f"  â€¢ ç”¨æˆ·åˆ’åˆ†æ¯”ä¾‹: {train_ratio:.1f}:{val_ratio:.1f}:{test_ratio:.1f}")
        
        # éªŒè¯æ¯”ä¾‹
        if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
            raise ValueError("ç”¨æˆ·åˆ’åˆ†æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1.0")
        
        # åŠ è½½æ•°æ®
        self._load_data(traffic_path, embedding_path)
        
        # æŒ‰ç”¨æˆ·åˆ’åˆ†
        self._split_users(train_ratio, val_ratio, test_ratio)
        
        # ç”Ÿæˆæ—¶é—´çª—å£æ ·æœ¬
        self._generate_samples()
        
        print(f"âœ… {mode} æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ:")
        print(f"  â€¢ åˆ†é…ç”¨æˆ·æ•°: {len(self.user_ids)}")
        print(f"  â€¢ ç”Ÿæˆæ ·æœ¬æ•°: {len(self.samples)}")
    
    def _load_data(self, traffic_path, embedding_path):
        """åŠ è½½æµé‡å’Œç¯å¢ƒæ•°æ®"""
        if not os.path.exists(traffic_path):
            self._create_sample_data(traffic_path, embedding_path)
        
        # åŠ è½½æµé‡æ•°æ® [n_users, n_apps, n_timesteps]
        traffic_data = np.load(traffic_path)
        self.traffic_matrix = traffic_data['traffic_matrix']
        
        # åŠ è½½ç¯å¢ƒåµŒå…¥ [n_users, n_timesteps, embedding_dim]
        if os.path.exists(embedding_path):
            env_data = np.load(embedding_path)
            self.env_embeddings = env_data['embeddings']
        else:
            # å¦‚æœç¯å¢ƒåµŒå…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºéšæœºåµŒå…¥
            print(f"âš ï¸ ç¯å¢ƒåµŒå…¥æ–‡ä»¶ {embedding_path} ä¸å­˜åœ¨ï¼Œåˆ›å»ºéšæœºåµŒå…¥")
            self.env_embeddings = np.random.randn(
                self.traffic_matrix.shape[0], 
                self.traffic_matrix.shape[2], 
                128
            )
        
        self.n_users, self.n_apps, self.n_timesteps = self.traffic_matrix.shape
        self.embedding_dim = self.env_embeddings.shape[2]
        
        print(f"ğŸ“Š æ•°æ®ç»´åº¦:")
        print(f"  â€¢ æ€»ç”¨æˆ·æ•°: {self.n_users}")
        print(f"  â€¢ åº”ç”¨æ•°é‡: {self.n_apps}")
        print(f"  â€¢ æ—¶é—´æ­¥æ•°: {self.n_timesteps}")
        print(f"  â€¢ åµŒå…¥ç»´åº¦: {self.embedding_dim}")
        
        # éªŒè¯æ—¶é—´ç»´åº¦
        total_required = self.hist_len + self.pred_len
        if total_required > self.n_timesteps:
            raise ValueError(
                f"æ‰€éœ€æ—¶é—´é•¿åº¦ ({total_required} = {self.hist_len}+{self.pred_len}) "
                f"> å¯ç”¨æ—¶é—´æ­¥æ•° ({self.n_timesteps})\n"
                f"è¯·å‡å°‘ hist_len æˆ– pred_len"
            )
    
    def _split_users(self, train_ratio, val_ratio, test_ratio):
        """æŒ‰ç”¨æˆ·ç»´åº¦åˆ’åˆ†æ•°æ®é›† - ç¡®ä¿æ— é‡å """
        print(f"ğŸ‘¥ æŒ‰ç”¨æˆ·åˆ’åˆ†æ•°æ®é›†...")
        
        # è®¾ç½®å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
        np.random.seed(42)
        all_user_ids = np.arange(self.n_users)
        np.random.shuffle(all_user_ids)
        
        # è®¡ç®—ç²¾ç¡®çš„åˆ’åˆ†ç‚¹
        train_end = int(np.floor(self.n_users * train_ratio))
        val_end = train_end + int(np.floor(self.n_users * val_ratio))
        
        # ç¡®ä¿æ‰€æœ‰ç”¨æˆ·éƒ½è¢«åˆ†é…
        if val_end >= self.n_users:
            val_end = self.n_users - 1
        
        # æŒ‰æ¨¡å¼é€‰æ‹©ç”¨æˆ·
        if self.mode == 'train':
            self.user_ids = all_user_ids[:train_end]
        elif self.mode == 'val':
            self.user_ids = all_user_ids[train_end:val_end]
        elif self.mode == 'test':
            self.user_ids = all_user_ids[val_end:]
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å¼: {self.mode}")
        
        # ç¡®ä¿æ¯ä¸ªé›†åˆè‡³å°‘æœ‰ä¸€ä¸ªç”¨æˆ·
        if len(self.user_ids) == 0:
            if self.mode == 'train':
                self.user_ids = all_user_ids[:max(1, self.n_users//2)]
            elif self.mode == 'val':
                start_idx = max(1, self.n_users//2)
                self.user_ids = all_user_ids[start_idx:start_idx+max(1, self.n_users//4)]
            else:  # test
                self.user_ids = all_user_ids[-max(1, self.n_users//4):]
        
        print(f"ğŸ“Š {self.mode} é›†åˆç”¨æˆ·åˆ†é…:")
        print(f"  â€¢ ç”¨æˆ·æ•°é‡: {len(self.user_ids)}")
        print(f"  â€¢ ç”¨æˆ·IDèŒƒå›´: [{self.user_ids.min()}, {self.user_ids.max()}]")
        print(f"  â€¢ å æ€»ç”¨æˆ·æ¯”ä¾‹: {len(self.user_ids)/self.n_users:.1%}")
    
    def _generate_samples(self):
        """ä¸ºåˆ†é…çš„ç”¨æˆ·ç”Ÿæˆæ»‘åŠ¨çª—å£æ ·æœ¬"""
        print(f"ğŸ”„ ä¸º {len(self.user_ids)} ä¸ªç”¨æˆ·ç”Ÿæˆæ ·æœ¬...")
        
        self.samples = []
        
        # è®¡ç®—æ—¶é—´çª—å£èŒƒå›´
        max_start_time = self.n_timesteps - self.hist_len - self.pred_len
        
        if max_start_time < 0:
            raise ValueError("æ—¶é—´ç»´åº¦ä¸è¶³ä»¥ç”Ÿæˆæ ·æœ¬")
        
        print(f"ğŸ“… æ—¶é—´çª—å£:")
        print(f"  â€¢ æœ€å¤§èµ·å§‹æ—¶é—´: {max_start_time}")
        print(f"  â€¢ æ»‘åŠ¨æ­¥é•¿: {self.stride}")
        
        total_samples = 0
        for user_id in self.user_ids:
            user_samples = 0
            
            # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆæ»‘åŠ¨çª—å£æ ·æœ¬
            for start_time in range(0, max_start_time + 1, self.stride):
                hist_start = start_time
                hist_end = start_time + self.hist_len
                pred_start = hist_end
                pred_end = pred_start + self.pred_len
                
                if pred_end <= self.n_timesteps:
                    self.samples.append({
                        'user_id': user_id,
                        'hist_start': hist_start,
                        'hist_end': hist_end,
                        'pred_start': pred_start,
                        'pred_end': pred_end
                    })
                    user_samples += 1
                    total_samples += 1
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯
            if user_id == self.user_ids[0]:
                print(f"  â€¢ ç”¨æˆ· {user_id} ç”Ÿæˆ {user_samples} ä¸ªæ ·æœ¬")
        
        print(f"  â€¢ æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"  â€¢ å¹³å‡æ¯ç”¨æˆ·: {total_samples/len(self.user_ids):.1f} æ ·æœ¬")
        
        if len(self.samples) == 0:
            raise ValueError("æ— æ³•ç”Ÿæˆä»»ä½•æœ‰æ•ˆæ ·æœ¬")
    
    def _create_sample_data(self, traffic_path, embedding_path):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
        print("ğŸ”§ åˆ›å»ºç¤ºä¾‹æµé‡æ•°æ®...")
        np.random.seed(42)
        
        # åˆ›å»ºæ›´çœŸå®çš„æµé‡æ•°æ®
        n_users, n_apps, n_timesteps = 871, 20, 168
        
        # ç”Ÿæˆå¸¦å‘¨æœŸæ€§çš„æµé‡æ•°æ®
        traffic_matrix = np.zeros((n_users, n_apps, n_timesteps))
        
        for user in range(n_users):
            for app in range(n_apps):
                # åŸºç¡€æµé‡æ°´å¹³
                base_traffic = np.random.exponential(1.0)
                
                # æ·»åŠ æ—¥å‘¨æœŸæ€§ (24å°æ—¶å‘¨æœŸ)
                hourly_pattern = np.sin(2 * np.pi * np.arange(n_timesteps) / 24) + 1
                
                # æ·»åŠ å‘¨å‘¨æœŸæ€§ (168å°æ—¶=7å¤©å‘¨æœŸ)
                weekly_pattern = 0.5 * np.cos(2 * np.pi * np.arange(n_timesteps) / 168) + 1
                
                # æ·»åŠ éšæœºå™ªå£°
                noise = np.random.normal(0, 0.3, n_timesteps)
                
                # ç»„åˆæ‰€æœ‰æˆåˆ†
                traffic_matrix[user, app, :] = (
                    base_traffic * hourly_pattern * weekly_pattern + noise
                ).clip(min=0)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(traffic_path).parent.mkdir(parents=True, exist_ok=True)
        Path(embedding_path).parent.mkdir(parents=True, exist_ok=True)
        
        np.savez(traffic_path, traffic_matrix=traffic_matrix)
        
        # åˆ›å»ºç¯å¢ƒåµŒå…¥
        env_embeddings = np.random.randn(n_users, n_timesteps, 128)
        np.savez(embedding_path, embeddings=env_embeddings)
        
        print("âœ… ç¤ºä¾‹æ•°æ®åˆ›å»ºå®Œæˆ")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        user_id = sample['user_id']
        
        # æå–å†å²æ•°æ® [hist_len, n_apps]
        hist_traffic = self.traffic_matrix[
            user_id, :, sample['hist_start']:sample['hist_end']
        ].T
        
        # æå–é¢„æµ‹ç›®æ ‡ [pred_len, n_apps]
        target_traffic = self.traffic_matrix[
            user_id, :, sample['pred_start']:sample['pred_end']
        ].T
        
        # æå–ç¯å¢ƒåµŒå…¥
        hist_env = self.env_embeddings[
            user_id, sample['hist_start']:sample['hist_end'], :
        ]
        
        pred_env = self.env_embeddings[
            user_id, sample['pred_start']:sample['pred_end'], :
        ]
        
        return {
            'hist_traffic': torch.from_numpy(hist_traffic).float(),
            'target_traffic': torch.from_numpy(target_traffic).float(),
            'hist_env': torch.from_numpy(hist_env).float(),
            'pred_env': torch.from_numpy(pred_env).float(),
            'user_id': user_id,
            'time_info': {
                'hist_range': (sample['hist_start'], sample['hist_end']),
                'pred_range': (sample['pred_start'], sample['pred_end']),
                'mode': self.mode
            }
        }


class TrafficCollator:
    """æµé‡æ•°æ®çš„æ‰¹å¤„ç†æ•´ç†å™¨"""
    
    def __init__(self, normalize=True, noise_std=0.01, task_params=None):
        self.normalize = normalize
        self.noise_std = noise_std
        self.task_params = task_params
    
    def collate(self, batch):
        """æ‰¹å¤„ç†æ•°æ®æ•´ç† - ä¿®å¤ç‰ˆæœ¬"""
        if len(batch) == 0:
            raise ValueError("Empty batch received!")
        
        batch_size = len(batch)
        
        # æå–æ•°æ®
        hist_traffic = torch.stack([item['hist_traffic'] for item in batch])  # (B, hist_len, n_apps)
        target_traffic = torch.stack([item['target_traffic'] for item in batch])  # (B, pred_len, n_apps)
        hist_env = torch.stack([item['hist_env'] for item in batch])  # (B, hist_len, embedding_dim)
        pred_env = torch.stack([item['pred_env'] for item in batch])  # (B, pred_len, embedding_dim)
        
        # print(f"ğŸ” åŸå§‹æ•°æ®å½¢çŠ¶:")
        # print(f"  â€¢ hist_traffic: {hist_traffic.shape}")
        # print(f"  â€¢ target_traffic: {target_traffic.shape}")
        # print(f"  â€¢ hist_env: {hist_env.shape}")
        
        # æ•°æ®æ ‡å‡†åŒ–
        if self.normalize:
            hist_mean = hist_traffic.mean(dim=(1, 2), keepdim=True)
            hist_std = hist_traffic.std(dim=(1, 2), keepdim=True) + 1e-8
            
            hist_traffic_norm = (hist_traffic - hist_mean) / hist_std
            target_traffic_norm = (target_traffic - hist_mean) / hist_std
            
            if self.noise_std > 0:
                noise = torch.randn_like(hist_traffic_norm) * self.noise_std
                hist_traffic_norm = hist_traffic_norm + noise
            
            hist_traffic_norm = torch.clamp(hist_traffic_norm, -10, 10)
            target_traffic_norm = torch.clamp(target_traffic_norm, -10, 10)
        else:
            hist_traffic_norm = hist_traffic
            target_traffic_norm = target_traffic
            hist_mean = torch.zeros_like(hist_traffic.mean(dim=1, keepdim=True))
            hist_std = torch.ones_like(hist_traffic.std(dim=1, keepdim=True))
        
        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®çš„æ•°æ®æ ¼å¼
        # ç›®æ ‡ï¼šè®©æ—¶é—´ç»´åº¦åœ¨æ­£ç¡®çš„ä½ç½®
        
        # æ–¹æ¡ˆA: ä¿æŒæ—¶é—´ç»´åº¦åœ¨ç¬¬äºŒä¸ªä½ç½® (æ¨è)
        # data: (B, time=pred_len, features=n_apps, 1)
        data = target_traffic_norm.unsqueeze(-1)  # (B, pred_len, n_apps, 1)
        
        # æ¡ä»¶æ•°æ®ï¼šç»„åˆå†å²æµé‡å’Œç¯å¢ƒåµŒå…¥
        if hist_traffic_norm.shape[1] != hist_env.shape[1]:
            min_seq_len = min(hist_traffic_norm.shape[1], hist_env.shape[1])
            hist_traffic_norm = hist_traffic_norm[:, :min_seq_len, :]
            hist_env = hist_env[:, :min_seq_len, :]
        
        # cond: (B, time=hist_len, features=n_apps+embedding_dim)
        combined_cond = torch.cat([hist_traffic_norm, hist_env], dim=-1)
        cond = combined_cond  # ä¿æŒ (B, hist_len, features) æ ¼å¼
        
        # print(f"ğŸ”§ ä¿®å¤åæ•°æ®å½¢çŠ¶:")
        # print(f"  â€¢ data: {data.shape} (B, pred_len={data.shape[1]}, n_apps={data.shape[2]}, 1)")
        # print(f"  â€¢ cond: {cond.shape} (B, hist_len={cond.shape[1]}, features={cond.shape[2]})")
        
        return {
            'data': data,  # (B, pred_len, n_apps, 1)
            'cond': cond,  # (B, hist_len, n_apps + embedding_dim)
            'pred_env': pred_env,
            'hist_traffic': hist_traffic,
            'target_traffic': target_traffic,
            'stats': {
                'mean': hist_mean,
                'std': hist_std
            },
            'user_ids': [item['user_id'] for item in batch],
            'time_info': [item['time_info'] for item in batch]
        }


def create_traffic_dataloaders_by_user(traffic_path="traffic_data.npz", 
                                      embedding_path="environment_embeddings.npz",
                                      batch_size=32, hist_len=148, pred_len=20, stride=1,
                                      train_ratio=0.6, val_ratio=0.2, test_ratio=0.2,
                                      num_workers=0, is_distributed=False):
    """
    åˆ›å»ºæŒ‰ç”¨æˆ·åˆ’åˆ†çš„æµé‡é¢„æµ‹æ•°æ®åŠ è½½å™¨
    """
    print("ğŸ—ï¸ åˆ›å»ºæŒ‰ç”¨æˆ·åˆ’åˆ†çš„æµé‡é¢„æµ‹æ•°æ®åŠ è½½å™¨...")
    print(f"ğŸ“‹ å‚æ•°é…ç½®:")
    print(f"  â€¢ ç”¨æˆ·åˆ’åˆ†æ¯”ä¾‹: {train_ratio:.1f}:{val_ratio:.1f}:{test_ratio:.1f}")
    print(f"  â€¢ å†å²é•¿åº¦: {hist_len}")
    print(f"  â€¢ é¢„æµ‹é•¿åº¦: {pred_len}")
    print(f"  â€¢ æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # åˆ›å»ºä¸‰ä¸ªæ•°æ®é›†
    datasets = {}
    for mode in ['train', 'val', 'test']:
        datasets[mode] = TrafficPredictionDataset(
            traffic_path=traffic_path,
            embedding_path=embedding_path,
            mode=mode,
            train_ratio=train_ratio,
            val_ratio=val_ratio, 
            test_ratio=test_ratio,
            hist_len=hist_len,
            pred_len=pred_len,
            stride=stride
        )
    
    # éªŒè¯ç”¨æˆ·ä¸é‡å 
    _validate_user_split(datasets)
    
    # åˆ›å»ºæ•°æ®æ•´ç†å™¨
    collator = TrafficCollator(normalize=True, noise_std=0.01)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloaders = {}
    for mode in ['train', 'val', 'test']:
        sampler = None
        shuffle = (mode == 'train')
        
        if is_distributed and mode == 'train':
            sampler = DistributedSampler(datasets[mode])
            shuffle = False
        
        dataloaders[mode] = DataLoader(
            datasets[mode],
            batch_size=min(batch_size, len(datasets[mode])),
            collate_fn=collator.collate,
            shuffle=shuffle,
            sampler=sampler,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=False
        )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ!")
    _print_dataset_summary(datasets, dataloaders)
    
    return dataloaders['train'], dataloaders['val'], dataloaders['test']


def _validate_user_split(datasets):
    """éªŒè¯ç”¨æˆ·åˆ’åˆ†æ²¡æœ‰é‡å """
    print("ğŸ” éªŒè¯ç”¨æˆ·åˆ’åˆ†...")
    
    train_users = set(datasets['train'].user_ids)
    val_users = set(datasets['val'].user_ids)
    test_users = set(datasets['test'].user_ids)
    
    # æ£€æŸ¥é‡å 
    overlaps = {
        'train_val': train_users & val_users,
        'train_test': train_users & test_users,
        'val_test': val_users & test_users
    }
    
    has_overlap = False
    for key, overlap in overlaps.items():
        if overlap:
            print(f"âŒ {key} ç”¨æˆ·é‡å : {len(overlap)} ä¸ªç”¨æˆ·")
            has_overlap = True
    
    if not has_overlap:
        print("âœ… éªŒè¯é€šè¿‡: ç”¨æˆ·åˆ’åˆ†å®Œå…¨æ— é‡å ")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    total_users = len(train_users) + len(val_users) + len(test_users)
    print(f"ğŸ“Š ç”¨æˆ·åˆ’åˆ†ç»Ÿè®¡:")
    print(f"  â€¢ è®­ç»ƒç”¨æˆ·: {len(train_users)} ({len(train_users)/total_users:.1%})")
    print(f"  â€¢ éªŒè¯ç”¨æˆ·: {len(val_users)} ({len(val_users)/total_users:.1%})")
    print(f"  â€¢ æµ‹è¯•ç”¨æˆ·: {len(test_users)} ({len(test_users)/total_users:.1%})")


def _print_dataset_summary(datasets, dataloaders):
    """æ‰“å°æ•°æ®é›†æ‘˜è¦"""
    print(f"ğŸ“Š æ•°æ®é›†è¯¦ç»†ç»Ÿè®¡:")
    for mode in ['train', 'val', 'test']:
        dataset = datasets[mode]
        loader = dataloaders[mode]
        print(f"  â€¢ {mode.upper()}:")
        print(f"    - ç”¨æˆ·æ•°: {len(dataset.user_ids)}")
        print(f"    - æ ·æœ¬æ•°: {len(dataset)}")
        print(f"    - æ‰¹æ¬¡æ•°: {len(loader)}")
        print(f"    - æ¯æ‰¹æ ·æœ¬: {loader.batch_size}")


def analyze_traffic_distribution(traffic_data):
    """åˆ†ææµé‡æ•°æ®åˆ†å¸ƒ"""
    print("\n" + "="*50)
    print("ğŸ“Š Traffic Data Analysis")
    print("="*50)
    
    print(f"Shape: {traffic_data.shape}")
    print(f"Mean: {traffic_data.mean():.4f}")
    print(f"Std: {traffic_data.std():.4f}")
    print(f"Min: {traffic_data.min():.4f}")
    print(f"Max: {traffic_data.max():.4f}")
    
    # æŒ‰åº”ç”¨ç±»åˆ«åˆ†æ
    app_names = [
        'Social Media', 'Video Streaming', 'Gaming', 'Web Browsing', 'Email',
        'Music Streaming', 'Navigation', 'Shopping', 'News', 'Weather',
        'Productivity', 'Education', 'Health', 'Finance', 'Travel',
        'Photography', 'Communication', 'Entertainment', 'Utilities', 'Others'
    ]
    
    print("\nPer-app statistics:")
    for i, app_name in enumerate(app_names[:min(len(app_names), traffic_data.shape[1])]):
        app_data = traffic_data[:, i, :]
        print(f"  {app_name}: mean={app_data.mean():.4f}, std={app_data.std():.4f}")


# å…¶ä»–æ•°æ®é›†ç±»ä¿æŒä¸å˜
class WiFiDataset(torch.utils.data.Dataset):
    def __init__(self, paths, embedding_path="env_info.npz", target_shape=(168, 10)):
        """
        paths: æ•°æ®æ–‡ä»¶å¤¹çš„è·¯å¾„åˆ—è¡¨
        embedding_path: å­˜å‚¨ embedding çš„ npz æ–‡ä»¶è·¯å¾„
        target_shape: æ•°æ®ç›®æ ‡å½¢çŠ¶
        """
        super().__init__()
        self.filenames = []
        self.target_shape = target_shape  # è®¾ç½®ç›®æ ‡å½¢çŠ¶

        # åŠ è½½æ‰€æœ‰ .mat æ–‡ä»¶è·¯å¾„
        for path in paths:
            self.filenames += glob(f'{path}/**/user_*.mat', recursive=True)

        if not self.filenames:
            raise ValueError(f"{paths} not found data")
        
        print(f"find {len(self.filenames)} files")
        
        # åŠ è½½ embedding ä¿¡æ¯
        if os.path.exists(embedding_path):
            self.embedding_data = np.load(embedding_path)['e']  # åŠ è½½å­—æ®µ e
            if len(self.embedding_data) != len(self.filenames):
                print(f"Warning: The number of embeddings ({len(self.embedding_data)}) does not match the number of data files ({len(self.filenames)}).")
                # å¤„ç†æ•°é‡ä¸åŒ¹é…çš„æƒ…å†µ
                min_len = min(len(self.embedding_data), len(self.filenames))
                self.filenames = self.filenames[:min_len]
                self.embedding_data = self.embedding_data[:min_len]
        else:
            print(f"Warning: Embedding file {embedding_path} not found, creating random embeddings")
            # åˆ›å»ºéšæœºåµŒå…¥ä½œä¸ºå¤‡é€‰
            self.embedding_data = np.random.randn(len(self.filenames), 128)

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        cur_filename = self.filenames[idx]
        
        try:
            # åŠ è½½å½“å‰æ•°æ®æ–‡ä»¶
            cur_sample = scio.loadmat(cur_filename, verify_compressed_data_integrity=False)
        
            if 'feature' not in cur_sample:
                raise KeyError(f"data {cur_filename} loss 'feature'")
            
            # åŠ è½½æ•°æ®å¹¶è½¬æ¢ä¸ºå®æ•°
            cur_data = torch.from_numpy(cur_sample['feature']).float()  # åŠ è½½ç‰¹å¾æ•°æ®å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°

            # åŠ è½½å¯¹åº”çš„ embedding æ•°æ®
            cur_cond = torch.from_numpy(self.embedding_data[idx]).float().squeeze(0)  # åŠ è½½å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°

            # è°ƒæ•´æ•°æ®çš„å¤§å°
            cur_data = self._resize_data(cur_data, self.target_shape)

            return {
                'data': cur_data,
                'cond': cur_cond
            }
        
        except Exception as e:
            print(f"Error loading file {cur_filename}: {e}")
            raise e

    def _resize_data(self, data, target_shape):
        """æ ¹æ®ç›®æ ‡å½¢çŠ¶è°ƒæ•´æ•°æ®çš„å¤§å°ï¼ˆå¡«å……æˆ–è£å‰ªï¼‰"""
        current_shape = data.shape
        
        if current_shape[0] < target_shape[0]:
            # æ•°æ®è¾ƒå°ï¼Œå¡«å……åˆ°ç›®æ ‡å½¢çŠ¶
            padding = (0, 0, 0, target_shape[0] - current_shape[0])  # å¡«å……åˆ°ç›®æ ‡è¡Œæ•°
            data = torch.nn.functional.pad(data, padding, "constant", 0)
        elif current_shape[0] > target_shape[0]:
            # æ•°æ®è¾ƒå¤§ï¼Œè£å‰ªåˆ°ç›®æ ‡å½¢çŠ¶
            data = data[:target_shape[0], :, :]

        # ç¡®ä¿åˆ—æ•°ç¬¦åˆç›®æ ‡å½¢çŠ¶ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
        if current_shape[1] != target_shape[1]:
            data = data[:, :target_shape[1], :]
        
        return data


class FMCWDataset(torch.utils.data.Dataset):
    def __init__(self, paths):
        super().__init__()
        self.filenames = []
        for path in paths:
            self.filenames += glob(f'{path}/**/*.mat', recursive=True)

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        cur_filename = self.filenames[idx]
        try:
            cur_sample = scio.loadmat(cur_filename)
            cur_data = torch.from_numpy(cur_sample['feature']).to(torch.complex64)
            cur_cond = torch.from_numpy(cur_sample['cond'].astype(np.int16)).to(torch.complex64)
            return {
                'data': cur_data,
                'cond': cur_cond.squeeze(0)
            }
        except Exception as e:
            print(f"Error loading file {cur_filename}: {e}")
            raise e


class MIMODataset(torch.utils.data.Dataset):
    def __init__(self, paths):
        super().__init__()
        self.filenames = []
        for path in paths:
            self.filenames += glob(f'{path}/**/*.mat', recursive=True)

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        dataset = scio.loadmat(self.filenames[idx])
        data = torch.from_numpy(dataset['down_link']).to(torch.complex64)
        cond = torch.from_numpy(dataset['up_link']).to(torch.complex64)
        return {
            'data': torch.view_as_real(data),
            'cond': torch.view_as_real(cond)
        }


class EEGDataset(torch.utils.data.Dataset):
    def __init__(self, paths):
        super().__init__()
        paths = paths[0]
        self.filenames = []
        self.filenames += glob(f'{paths}/*.mat', recursive=True)

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        dataset = scio.loadmat(self.filenames[idx])
        data = torch.from_numpy(dataset['clean']).to(torch.complex64)
        cond = torch.from_numpy(dataset['disturb']).to(torch.complex64)
        return {
            'data': data,
            'cond': cond
        }


class Collator:
    def __init__(self, params):
        self.params = params

    def collate(self, minibatch):
        sample_rate = self.params.sample_rate
        task_id = self.params.task_id

        # è¿‡æ»¤æ‰æ— æ•ˆçš„è®°å½•
        valid_batch = [record for record in minibatch if 'data' in record and 'cond' in record]
        
        if len(valid_batch) == 0:
            raise ValueError("No valid records in batch!")

        # å¤„ç† WiFi Case
        if task_id == 0:
            processed_batch = []
            for record in valid_batch:
                if len(record['data']) < sample_rate:
                    continue  # è·³è¿‡é•¿åº¦ä¸è¶³çš„è®°å½•
                record['data'] = record['data'].unsqueeze(0)
                data = record['data'].permute(1, 2, 0)
                down_sample = F.interpolate(data, sample_rate, mode='nearest-exact')
                norm_data = (down_sample - down_sample.mean()) / (down_sample.std() + 1e-8)
                record['data'] = norm_data.permute(2, 0, 1)
                processed_batch.append(record)
            
            if len(processed_batch) == 0:
                raise ValueError("No valid records after processing!")
                
            data = torch.stack([record['data'] for record in processed_batch])
            cond = torch.stack([record['cond'] for record in processed_batch])
            return {
                'data': data,
                'cond': cond,
            }

        # å¤„ç† FMCW Case
        elif task_id == 1:
            processed_batch = []
            for record in valid_batch:
                if len(record['data']) < sample_rate:
                    continue
                data = torch.view_as_real(record['data']).permute(1, 2, 0)
                down_sample = F.interpolate(data, sample_rate, mode='nearest-exact')
                norm_data = (down_sample - down_sample.mean()) / (down_sample.std() + 1e-8)
                record['data'] = norm_data.permute(2, 0, 1)
                processed_batch.append(record)
                
            if len(processed_batch) == 0:
                raise ValueError("No valid records after processing!")
                
            data = torch.stack([record['data'] for record in processed_batch])
            cond = torch.stack([record['cond'] for record in processed_batch])
            return {
                'data': data,
                'cond': torch.view_as_real(cond),
            }

        # å¤„ç† MIMO Case
        elif task_id == 2:
            processed_batch = []
            for record in valid_batch:
                data = record['data']
                cond = record['cond']
                cond_std = cond.std() + 1e-8
                norm_data = (data) / cond_std
                norm_cond = (cond) / cond_std
                record['data'] = norm_data.reshape(14, 96, 26, 2).transpose(1, 2)
                record['cond'] = norm_cond.reshape(14, 96, 26, 2).transpose(1, 2)
                processed_batch.append(record)
                
            data = torch.stack([record['data'] for record in processed_batch])
            cond = torch.stack([record['cond'] for record in processed_batch])
            return {
                'data': data,
                'cond': cond,
            }

        # å¤„ç† EEG Case
        elif task_id == 3:
            processed_batch = []
            for record in valid_batch:
                data = record['data']
                cond = record['cond']
                cond_std = cond.std() + 1e-8
                norm_data = data / cond_std
                norm_cond = cond / cond_std
                record['data'] = norm_data.reshape(512, 1, 1)
                record['cond'] = norm_cond.reshape(512)
                processed_batch.append(record)
                
            data = torch.stack([record['data'] for record in processed_batch])
            cond = torch.stack([record['cond'] for record in processed_batch])
            return {
                'data': torch.view_as_real(data),
                'cond': torch.view_as_real(cond),
            }
        
        # å¤„ç†æµé‡é¢„æµ‹ Case - è¿™é‡Œåº”è¯¥ä¸ä¼šè¢«è°ƒç”¨ï¼Œå› ä¸ºæœ‰ä¸“é—¨çš„TrafficCollator
        elif task_id == 4:
            # å¤‡ç”¨å¤„ç†é€»è¾‘
            data = torch.stack([record['target_traffic'] for record in valid_batch])
            cond = torch.stack([record['hist_traffic'] for record in valid_batch])
            return {
                'data': data.permute(0, 2, 1).unsqueeze(-1),  # (B, n_apps, pred_len, 1)
                'cond': cond.permute(0, 2, 1)  # (B, n_apps, hist_len)
            }

        else:
            raise ValueError("Unexpected task_id.")


def from_path(params, is_distributed=False):
    """ç»Ÿä¸€çš„æ•°æ®é›†åˆ›å»ºå‡½æ•°"""
    data_dir = params.data_dir
    task_id = params.task_id
    
    print(f"ğŸ”§ Creating dataset for task_id: {task_id}")
    
    if task_id == 4:  # æµé‡é¢„æµ‹ä»»åŠ¡
        print("ğŸ“Š Creating traffic prediction dataset with USER-BASED split...")
        
        try:
            if hasattr(params, 'data_dir') and params.data_dir:
                data_dir = Path(params.data_dir[0]) if isinstance(params.data_dir, list) else Path(params.data_dir)
            else:
                data_dir = Path("dataset/traffic")
            
            traffic_path = data_dir / getattr(params, 'traffic_path', 'traffic_data_new.npz')
            embedding_path = getattr(params, 'embedding_path', 'environment_embeddings.npz')
            
            print(f"ğŸ” Data paths:")
            print(f"  â€¢ Traffic: {traffic_path}")
            print(f"  â€¢ Embedding: {embedding_path}")
            
            # ğŸ”§ é‡è¦ï¼šç¡®ä¿ä½¿ç”¨æŒ‰ç”¨æˆ·åˆ’åˆ†çš„æ•°æ®é›†
            print("ğŸš¨ ä½¿ç”¨æŒ‰ç”¨æˆ·åˆ’åˆ†çš„æ•°æ®é›†ï¼ˆUSER-BASED SPLITï¼‰")
            
            # ç›´æ¥åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼Œè€Œä¸æ˜¯æ•°æ®åŠ è½½å™¨
            train_dataset = TrafficPredictionDataset(
                traffic_path=str(traffic_path),
                embedding_path=str(embedding_path),
                mode='train',
                train_ratio=0.6,
                val_ratio=0.2,
                test_ratio=0.2,
                hist_len=getattr(params, 'input_seq_len', 148),
                pred_len=getattr(params, 'pred_len', 20),
                stride=1
            )
            
            # åˆ›å»ºæ•°æ®æ•´ç†å™¨
            collator = TrafficCollator(normalize=True, noise_std=0.01)
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            dataloader = torch.utils.data.DataLoader(
                train_dataset,
                batch_size=params.batch_size,
                collate_fn=collator.collate,
                shuffle=not is_distributed,
                num_workers=0,
                sampler=DistributedSampler(train_dataset) if is_distributed else None,
                pin_memory=True,
                drop_last=False,
                persistent_workers=False
            )
            
            print(f"âœ… USER-BASED traffic dataset created successfully!")
            print(f"ğŸ“Š Training dataset info:")
            print(f"  â€¢ Users: {len(train_dataset.user_ids)}")
            print(f"  â€¢ Samples: {len(train_dataset)}")
            print(f"  â€¢ Batches: {len(dataloader)}")
            
            return dataloader
            
        except Exception as e:
            print(f"âŒ Error creating traffic dataset: {e}")
            raise e
    
    # å…¶ä»–ä»»åŠ¡çš„å¤„ç†
    elif task_id == 0:
        dataset = WiFiDataset(data_dir)
    elif task_id == 1:
        dataset = FMCWDataset(data_dir)
    elif task_id == 2:
        dataset = MIMODataset(data_dir)
    elif task_id == 3:
        dataset = EEGDataset(data_dir)
    else:
        raise ValueError("Unexpected task_id.")
    
    # åˆ›å»ºæ ‡å‡†æ•°æ®åŠ è½½å™¨
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=params.batch_size,
        collate_fn=Collator(params).collate,
        shuffle=not is_distributed,
        num_workers=0,
        sampler=DistributedSampler(dataset) if is_distributed else None,
        pin_memory=True,
        drop_last=True,
        persistent_workers=False
    )
    
    return dataloader


def from_path_inference(params):
    """æ¨ç†æ—¶çš„æ•°æ®é›†åˆ›å»ºå‡½æ•°"""
    cond_dir = params.cond_dir
    task_id = params.task_id
    
    if task_id == 0:
        dataset = WiFiDataset(cond_dir)
    elif task_id == 1:
        dataset = FMCWDataset(cond_dir)
    elif task_id == 2:
        dataset = MIMODataset(cond_dir)
    elif task_id == 3:
        dataset = EEGDataset(cond_dir)
    elif task_id == 4:
        # æµé‡é¢„æµ‹æ¨ç†
        dataset = TrafficPredictionDataset(
            traffic_path=params.traffic_path,
            embedding_path=params.embedding_path,
            mode='test'
        )
    else:
        raise ValueError("Unexpected task_id.")
    
    return torch.utils.data.DataLoader(
        dataset,
        batch_size=1,
        collate_fn=Collator(params).collate if task_id != 4 else TrafficCollator().collate,
        shuffle=False,
        num_workers=0,
        pin_memory=True,
        drop_last=True,
        persistent_workers=False
    )


# è¾…åŠ©å‡½æ•°
def _nested_map(struct, map_fn):
    if isinstance(struct, tuple):
        return tuple(_nested_map(x, map_fn) for x in struct)
    if isinstance(struct, list):
        return [_nested_map(x, map_fn) for x in struct]
    if isinstance(struct, dict):
        return {k: _nested_map(v, map_fn) for k, v in struct.items()}
    return map_fn(struct)


# æµ‹è¯•å‡½æ•°
def test_traffic_dataset():
    """æµ‹è¯•æµé‡æ•°æ®é›†"""
    print("ğŸ§ª Testing TrafficDataset...")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    try:
        train_loader, val_loader, test_loader = create_traffic_dataloaders_by_user(
            batch_size=4, hist_len=12, pred_len=6, num_workers=0
        )
    except Exception as e:
        print(f"âŒ Error creating dataloaders: {e}")
        return None, None, None
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    print(f"\nğŸ” Testing data loading...")
    try:
        batch = next(iter(train_loader))
        
        print(f"âœ… Batch loaded successfully!")
        print(f"ğŸ“Š Batch info:")
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                print(f"  â€¢ {key}: {value.shape} ({value.dtype})")
            else:
                print(f"  â€¢ {key}: {type(value)}")
        
        return train_loader, val_loader, test_loader
    except Exception as e:
        print(f"âŒ Error loading batch: {e}")
        return None, None, None


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    train_loader, val_loader, test_loader = test_traffic_dataset()
    if train_loader is not None:
        print("\nğŸ‰ All tests passed!")
    else:
        print("\nâŒ Tests failed!")
